


# ğŸ§  Materialsâ€‘Science Domainâ€‘Adapted LLaMAâ€¯3â€¯8B (Prof. A.K.â€¯Singh Corpus)

This repository contains a **domain-adapted LLaMAâ€¯3â€¯8B** model, further pre-trained on **223 materialsâ€‘science papers** authored by Prof. Abhishek K. Singh.

---

## ğŸ“Š Perplexity Results

- **Perplexity:** 9.4790  
- **Tokens evaluated:** 51,100  
- **Avg. tokens per sample:** 511.0

## ğŸ“ˆ Training vs. Validation Loss

The figure below shows the modelâ€™s learning curves during domain-adaptive pretraining:

![Training and Validation Loss](train_test_loss.png)


## ğŸ§ª Workflow Overview

- **`pdf_to_txt_data_extract.ipynb`**  
  Extracts text from PDF research papers into `.txt` files.

- **`hf_dataprep.ipynb`**  
  Preprocesses raw text files into chunks (e.g., 1024 tokens) suitable for training.

- **`dapt-final-aks-mrc.ipynb`**  
  Full pipeline: loads base LLaMAâ€¯3â€¯8B, dataset prep, DAPT, evaluation, merging, and model publishing to Hugging Face Spaces, plus inference.

- **`train_test_loss.png`**  
  Visualizes train/validation loss curves during DAPT.



## ğŸš€ Usage

1. **Clone the repo**  
   ```bash
   git clone https://github.com/harshu0117/Materials_IISC_AKS.git
   cd Materials_IISC_AKS


2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Follow notebooks** in order:

   * `pdf_to_txt_data_extract.ipynb` â†’ raw text extraction
   * `hf_dataprep.ipynb` â†’ chunked dataset creation
   * `dapt-final-aks-mrc.ipynb` â†’ model training, evaluation, merge, and deployment

4. **Load the model** in Hugging Face Transformers:

   ```python
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    # Load your model from Hugging Face
    model_name = "Harshu0117/Materials_IISC_MRC"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

   ```

5. **Inference example**:

   ```python
   # Set pad token if not set
    if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
    
    # Inference
    inputs = tokenizer("Crystalline MAX Phases and their 2D derivative MXenes", return_tensors="pt").to("cuda")
    
    outputs = model.generate(
       **inputs,
       max_new_tokens=200,
       repetition_penalty=1.2,
       temperature=0.8,
       top_k=50,
       top_p=0.95,
       do_sample=True,
       pad_token_id=tokenizer.pad_token_id
    )
    
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
   ```

---

## ğŸ¯ Why It Matters

Continuing pretraining (DAPT) on Prof. Singhâ€™s materialsâ€‘science corpus allows the model to **better understand domainâ€‘specific language**, leading to improved performance on tasks like technical summarization, Q\&A, and synthesis of scientific content.

---

## ğŸ“‚ File Structure

```
Materials_IISC_AKS/
â”œâ”€â”€ pdf_to_txt_data_extract.ipynb
â”œâ”€â”€ hf_dataprep.ipynb
â”œâ”€â”€ dapt-final-aks-mrc.ipynb
â”œâ”€â”€ train_test_loss.png
â””â”€â”€ requirements.txt
```

---

## âœ¨ Get Involved

* Use the model in your research or applications.
* Open an issue if you'd like to add evaluation benchmarks or new features.
* Star â­ï¸ the repo to show support!

---

## ğŸ“ Citation

If you use this model, please cite:

```
@misc{aks_llama3_dapt,
  title = {Domainâ€‘Adaptive Pretraining of LLaMAâ€‘3 8B on Materialsâ€‘Science Corpus by A.K. Singh},
  author = {Harshu and Abhishek K. Singh},
  howpublished = {GitHub repository},
  year = {2025},
  url = {https://github.com/harshu0117/Materials_IISC_AKS}
}
```

## ğŸ“œ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---


