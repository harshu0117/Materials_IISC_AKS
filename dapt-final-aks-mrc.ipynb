{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T00:16:02.338722Z",
     "iopub.status.busy": "2025-07-11T00:16:02.338563Z",
     "iopub.status.idle": "2025-07-11T00:16:02.685509Z",
     "shell.execute_reply": "2025-07-11T00:16:02.684630Z",
     "shell.execute_reply.started": "2025-07-11T00:16:02.338707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T00:16:02.686721Z",
     "iopub.status.busy": "2025-07-11T00:16:02.686355Z",
     "iopub.status.idle": "2025-07-11T00:16:02.692696Z",
     "shell.execute_reply": "2025-07-11T00:16:02.691894Z",
     "shell.execute_reply.started": "2025-07-11T00:16:02.686701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Environment Setup & GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:36:24.009450Z",
     "iopub.status.busy": "2025-07-10T15:36:24.009101Z",
     "iopub.status.idle": "2025-07-10T15:36:28.955605Z",
     "shell.execute_reply": "2025-07-10T15:36:28.954783Z",
     "shell.execute_reply.started": "2025-07-10T15:36:24.009431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "Current device: 0\n",
      "GPU 0: Tesla T4\n",
      "GPU 0 Memory: 15.8GB\n",
      "GPU 1: Tesla T4\n",
      "GPU 1 Memory: 15.8GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check GPU setup\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory fraction to prevent OOM\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8, i)\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Install Dependencies\n",
    "\n",
    "## Turn on the Internet on kaggel\n",
    "\n",
    "Make sure you update transformers according to unsloth's latest requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:36:38.738717Z",
     "iopub.status.busy": "2025-07-10T15:36:38.738447Z",
     "iopub.status.idle": "2025-07-10T15:38:20.236826Z",
     "shell.execute_reply": "2025-07-10T15:38:20.236093Z",
     "shell.execute_reply.started": "2025-07-10T15:36:38.738695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hâœ… Transformers upgraded successfully (quiet mode)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… Unsloth Zoo installed successfully (quiet mode)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ… Unsloth installed successfully (quiet mode)\n",
      "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
      "âœ… Packages installed successfully (quiet mode, no dependencies)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade \"transformers>=4.53.0\"\n",
    "print(\"âœ… Transformers upgraded successfully (quiet mode)\")\n",
    "\n",
    "!pip install -q unsloth_zoo\n",
    "print(\"âœ… Unsloth Zoo installed successfully (quiet mode)\")\n",
    "\n",
    "!pip install -q git+https://github.com/unslothai/unsloth.git \n",
    "print(\"âœ… Unsloth installed successfully (quiet mode)\")\n",
    "\n",
    "!pip install -q --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\"\n",
    "print(\"âœ… Packages installed successfully (quiet mode, no dependencies)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:38:24.104719Z",
     "iopub.status.busy": "2025-07-10T15:38:24.104431Z",
     "iopub.status.idle": "2025-07-10T15:38:29.894344Z",
     "shell.execute_reply": "2025-07-10T15:38:29.893580Z",
     "shell.execute_reply.started": "2025-07-10T15:38:24.104693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:38:29.895965Z",
     "iopub.status.busy": "2025-07-10T15:38:29.895680Z",
     "iopub.status.idle": "2025-07-10T15:39:08.916398Z",
     "shell.execute_reply": "2025-07-10T15:39:08.915699Z",
     "shell.execute_reply.started": "2025-07-10T15:38:29.895943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 15:38:40.616329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752161920.856722      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752161920.920858      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth version: 2025.7.2\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import unsloth\n",
    "print(f\"Unsloth version: {unsloth.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Configure Multi-GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:39:16.112656Z",
     "iopub.status.busy": "2025-07-10T15:39:16.112356Z",
     "iopub.status.idle": "2025-07-10T15:39:19.554037Z",
     "shell.execute_reply": "2025-07-10T15:39:19.552953Z",
     "shell.execute_reply.started": "2025-07-10T15:39:16.112632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:39:21.952529Z",
     "iopub.status.busy": "2025-07-10T15:39:21.951809Z",
     "iopub.status.idle": "2025-07-10T15:39:22.097224Z",
     "shell.execute_reply": "2025-07-10T15:39:22.096565Z",
     "shell.execute_reply.started": "2025-07-10T15:39:21.952492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 2\n",
      "CUDA available: True\n",
      "ğŸš€ Multiple GPUs detected - setting up distributed training\n",
      "âœ… Setup successful!\n",
      "Process index: 0\n",
      "Device: cuda\n",
      "Distributed type: DistributedType.NO\n",
      "Num processes: 1\n",
      "âŒ Setup failed: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
      "ğŸ’¡ Try: pip install accelerate wandb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "# Quick GPU check\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check if distributed training is possible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"ğŸš€ Multiple GPUs detected - setting up distributed training\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Use both GPUs\n",
    "else:\n",
    "    print(\"Single GPU mode\")\n",
    "\n",
    "# Your code with fixes\n",
    "try:\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"fp16\",\n",
    "        log_with=\"wandb\" if os.environ.get('WANDB_API_KEY') else None,\n",
    "    )\n",
    "    \n",
    "    set_seed(42)\n",
    "    \n",
    "    # Results\n",
    "    print(f\"âœ… Setup successful!\")\n",
    "    print(f\"Process index: {accelerator.process_index}\")\n",
    "    print(f\"Device: {accelerator.device}\")\n",
    "    print(f\"Distributed type: {accelerator.distributed_type}\")\n",
    "    print(f\"Num processes: {accelerator.num_processes}\")\n",
    "    \n",
    "    # Test with a simple model - IMPORTANT: prepare model AND data together\n",
    "    model = torch.nn.Linear(10, 1)\n",
    "    \n",
    "    # Create dummy data\n",
    "    test_input = torch.randn(2, 10)\n",
    "    \n",
    "    # Prepare model and data together - this fixes the device mismatch\n",
    "    model, test_input = accelerator.prepare(model, test_input)\n",
    "    \n",
    "    output = model(test_input)\n",
    "    print(f\"Model test passed âœ…\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Input device: {test_input.device}\")\n",
    "    \n",
    "    # Final verdict\n",
    "    if accelerator.num_processes > 1:\n",
    "        print(f\"ğŸ‰ Multi-GPU working with {accelerator.num_processes} GPUs!\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Single GPU/CPU mode (normal for Kaggle)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Setup failed: {e}\")\n",
    "    print(\"ğŸ’¡ Try: pip install accelerate wandb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Load Model with Unsloth\n",
    "\n",
    "#### Change the model name accordingly to your use case , we are using llama 3 8B ,\n",
    "\n",
    "#### For kaggel this is the best fit, if your running locally with GPU you can upgrade the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:51:55.996286Z",
     "iopub.status.busy": "2025-07-10T15:51:55.995522Z",
     "iopub.status.idle": "2025-07-10T15:52:29.622213Z",
     "shell.execute_reply": "2025-07-10T15:52:29.621449Z",
     "shell.execute_reply.started": "2025-07-10T15:51:55.996250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 15:52:00.879361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752162720.902707     443 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752162720.909962     443 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.7.2: Fast Llama patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"unsloth/llama-3-8b-bnb-4bit\"  # Change as needed\n",
    "max_seq_length = 2048                 \n",
    "dtype = None  # Auto-detect  # dtype = torch.float16\n",
    "load_in_4bit = True  # load_in_4bit = False\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you have the model weights to be in torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:52:29.623424Z",
     "iopub.status.busy": "2025-07-10T15:52:29.623169Z",
     "iopub.status.idle": "2025-07-10T15:52:29.633815Z",
     "shell.execute_reply": "2025-07-10T15:52:29.633028Z",
     "shell.execute_reply.started": "2025-07-10T15:52:29.623403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 335,544,320 || all params: 8,365,805,568 || trainable%: 4.0109\n",
      "Trainable parameters: None\n",
      "\n",
      "Model weights inspection:\n",
      "base_model.model.model.embed_tokens.weight: torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n",
    "\n",
    "# Check model weights dtype\n",
    "print(\"\\nModel weights inspection:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "    if \"lm_head\" in name or \"embed\" in name:  # Check key layers\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Dataset Loading and Preprocessing\n",
    "\n",
    "#### Remember to change the dataset to your dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T23:58:59.993048Z",
     "iopub.status.busy": "2025-07-10T23:58:59.992449Z",
     "iopub.status.idle": "2025-07-10T23:59:03.771236Z",
     "shell.execute_reply": "2025-07-10T23:59:03.770618Z",
     "shell.execute_reply.started": "2025-07-10T23:58:59.993024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1533 samples\n",
      "After filtering: 1533 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc09c57ded14c24bf917fa77bdad0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1533 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1379\n",
      "Eval samples: 154\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Simple tokenization function\"\"\"\n",
    "    try:\n",
    "        # Tokenize with padding and truncation\n",
    "        tokens = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        # Return empty tokens on error\n",
    "        return {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "# Load your dataset - replace with your HF dataset name\n",
    "dataset_name = \"Harshu0117/AKS_IISC_1024_processed\"  # Change this\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    # Basic filtering for empty/malformed text\n",
    "    dataset = dataset.filter(lambda x: x[\"text\"] and len(x[\"text\"].strip()) > 10)\n",
    "    print(f\"After filtering: {len(dataset)} samples\")\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "    \n",
    "    # Split for validation\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Dataset loading error: {e}\")\n",
    "    # Fallback to dummy data for testing\n",
    "    print(\"Using dummy data for testing\")\n",
    "    dummy_data = {\"text\": [\"This is a test sentence.\"] * 100}\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    train_dataset = eval_dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tokenized dataset should only contain `['input_ids', 'attention_mask', 'labels']` \n",
    "\n",
    "#### remove that col not the text cols, if it contains make sure to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:52:42.634422Z",
     "iopub.status.busy": "2025-07-10T15:52:42.633588Z",
     "iopub.status.idle": "2025-07-10T15:52:42.639919Z",
     "shell.execute_reply": "2025-07-10T15:52:42.639277Z",
     "shell.execute_reply.started": "2025-07-10T15:52:42.634386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: Training Configuration with OOM (Out of Memory) Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I would highly encourage you to tweak the  `training_args` parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:52:47.075696Z",
     "iopub.status.busy": "2025-07-10T15:52:47.075110Z",
     "iopub.status.idle": "2025-07-10T15:52:47.120436Z",
     "shell.execute_reply": "2025-07-10T15:52:47.119698Z",
     "shell.execute_reply.started": "2025-07-10T15:52:47.075669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "- Batch size per device: 2\n",
      "- Gradient accumulation: 4\n",
      "- Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import math\n",
    "\n",
    "# OOM handling function\n",
    "def handle_oom_error(batch_size):\n",
    "    \"\"\"Reduce batch size on OOM\"\"\"\n",
    "    new_batch_size = max(1, batch_size // 2)\n",
    "    print(f\"OOM detected! Reducing batch size from {batch_size} to {new_batch_size}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return new_batch_size\n",
    "\n",
    "# Dynamic batch size configuration\n",
    "initial_batch_size = 2 if torch.cuda.device_count() > 1 else 2\n",
    "current_batch_size = initial_batch_size\n",
    "\n",
    "# Training arguments with multi-GPU support\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/results\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=current_batch_size,\n",
    "    per_device_eval_batch_size=current_batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",   # comment out if dosen't work... \n",
    "    eval_steps=30,  ## set to 100 , if you have too many steps (600-800 etc) \n",
    "    save_steps=210,   ## should be a multiple of eval_steps\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    ddp_find_unused_parameters=False,  # Important for multi-GPU\n",
    "    dataloader_pin_memory=False,  # Reduce memory usage\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    resume_from_checkpoint=None,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"- Batch size per device: {current_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Effective batch size: {current_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: Custom Trainer with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:52:52.246479Z",
     "iopub.status.busy": "2025-07-10T15:52:52.245859Z",
     "iopub.status.idle": "2025-07-10T15:52:52.255088Z",
     "shell.execute_reply": "2025-07-10T15:52:52.254392Z",
     "shell.execute_reply.started": "2025-07-10T15:52:52.246453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trainer configured with OOM and NaN handling\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "class SafeTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.oom_count = 0\n",
    "        self.max_oom_retries = 3\n",
    "        \n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"Override training step with OOM handling\"\"\"\n",
    "        try:\n",
    "            return super().training_step(model, inputs, num_items_in_batch)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                self.oom_count += 1\n",
    "                print(f\"OOM error #{self.oom_count}: {e}\")\n",
    "                \n",
    "                if self.oom_count <= self.max_oom_retries:\n",
    "                    # Clear cache and retry with smaller batch\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Reduce batch size\n",
    "                    current_batch_size = self.args.per_device_train_batch_size\n",
    "                    new_batch_size = handle_oom_error(current_batch_size)\n",
    "                    self.args.per_device_train_batch_size = new_batch_size\n",
    "                    self.args.per_device_eval_batch_size = new_batch_size\n",
    "                    \n",
    "                    # Try again\n",
    "                    return super().training_step(model, inputs, num_items_in_batch)\n",
    "                else:\n",
    "                    print(\"Max OOM retries reached. Stopping training.\")\n",
    "                    raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override loss computation with NaN detection - Fixed signature\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: NaN/Inf loss detected: {loss}\")\n",
    "            # Return a small positive loss to continue training\n",
    "            loss = torch.tensor(0.01, device=loss.device, requires_grad=True)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Not using masked language modeling\n",
    "    pad_to_multiple_of=8,  # Efficiency for tensor cores\n",
    ")\n",
    "\n",
    "print(\"Custom trainer configured with OOM and NaN handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just making sure the input is the exactly same dimension as defined , this function will \n",
    "\n",
    "#### trim the inuts if they are larger or pad them accordingly\n",
    "\n",
    "> This was actually built for 2048 ( which is very standard for small model pre trainning , but im going with 1024 due to OOM issues, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:52:57.126076Z",
     "iopub.status.busy": "2025-07-10T15:52:57.125493Z",
     "iopub.status.idle": "2025-07-10T15:52:57.182209Z",
     "shell.execute_reply": "2025-07-10T15:52:57.181476Z",
     "shell.execute_reply.started": "2025-07-10T15:52:57.126051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset validation...\n",
      "Final validation complete!\n",
      "\n",
      "=== TRAIN LENGTH CHECK ===\n",
      "input_ids: 1024\n",
      "attention_mask: 1024\n",
      "labels: 1024\n",
      "WARNING: Example 0 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 1 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 2 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 3 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 4 has inconsistent lengths: [1024, 1024, 1024]\n",
      "\n",
      "=== EVAL LENGTH CHECK ===\n",
      "input_ids: 1024\n",
      "attention_mask: 1024\n",
      "labels: 1024\n",
      "WARNING: Example 0 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 1 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 2 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 3 has inconsistent lengths: [1024, 1024, 1024]\n",
      "WARNING: Example 4 has inconsistent lengths: [1024, 1024, 1024]\n"
     ]
    }
   ],
   "source": [
    "def validate_and_fix_dataset(dataset, max_length=1024):\n",
    "    \"\"\"Final validation and fixing of dataset\"\"\"\n",
    "    def validate_example(example):\n",
    "        # Ensure exact length for all sequences\n",
    "        for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "            if key in example:\n",
    "                seq = example[key]\n",
    "                if len(seq) > max_length:\n",
    "                    seq = seq[:max_length]\n",
    "                elif len(seq) < max_length:\n",
    "                    if key == 'input_ids':\n",
    "                        pad_value = 128001  # Use your tokenizer's pad_token_id\n",
    "                    elif key == 'attention_mask':\n",
    "                        pad_value = 0\n",
    "                    else:  # labels\n",
    "                        pad_value = -100\n",
    "                    \n",
    "                    seq = seq + [pad_value] * (max_length - len(seq))\n",
    "                \n",
    "                example[key] = seq\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    return dataset.map(validate_example, num_proc=4)\n",
    "\n",
    "# Apply final validation\n",
    "print(\"Final dataset validation...\")\n",
    "train_dataset = validate_and_fix_dataset(train_dataset)\n",
    "eval_dataset = validate_and_fix_dataset(eval_dataset)\n",
    "print(\"Final validation complete!\")\n",
    "\n",
    "# Verify all sequences are exactly 2048 tokens -- this was originally built for 2048 tokens \n",
    "def check_lengths(dataset, name):\n",
    "    print(f\"\\n=== {name} LENGTH CHECK ===\")\n",
    "    sample = dataset[0]\n",
    "    for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "        if key in sample:\n",
    "            print(f\"{key}: {len(sample[key])}\")\n",
    "    \n",
    "    # Check a few more examples\n",
    "    for i in range(min(5, len(dataset))):\n",
    "        example = dataset[i]\n",
    "        lengths = [len(example[key]) for key in ['input_ids', 'attention_mask', 'labels']]\n",
    "        if not all(l == 2048 for l in lengths):\n",
    "            print(f\"WARNING: Example {i} has inconsistent lengths: {lengths}\")\n",
    "        else:\n",
    "            print(f\"Example {i}: All lengths = 2048 âœ“\")\n",
    "\n",
    "check_lengths(train_dataset, \"TRAIN\")\n",
    "check_lengths(eval_dataset, \"EVAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This above warning is ok , its jut saying my tensor lengths are 1024, nothing to woory about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8: Initialize and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T23:39:23.144639Z",
     "iopub.status.busy": "2025-07-10T23:39:23.144356Z",
     "iopub.status.idle": "2025-07-10T23:39:23.148508Z",
     "shell.execute_reply": "2025-07-10T23:39:23.147768Z",
     "shell.execute_reply.started": "2025-07-10T23:39:23.144619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T15:53:05.542440Z",
     "iopub.status.busy": "2025-07-10T15:53:05.541852Z",
     "iopub.status.idle": "2025-07-10T23:19:45.540630Z",
     "shell.execute_reply": "2025-07-10T23:19:45.539816Z",
     "shell.execute_reply.started": "2025-07-10T15:53:05.542412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443/2955143457.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SafeTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,379 | Num Epochs = 4 | Total steps = 348\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 335,544,320 of 8,365,805,568 (4.01% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 7:25:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.301000</td>\n",
       "      <td>2.140013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.860100</td>\n",
       "      <td>2.111856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>8.031400</td>\n",
       "      <td>2.116166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.999300</td>\n",
       "      <td>2.103918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>8.024700</td>\n",
       "      <td>2.089628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.465600</td>\n",
       "      <td>2.075586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>7.805800</td>\n",
       "      <td>2.069516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>2.064974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>7.467800</td>\n",
       "      <td>2.062554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7.319100</td>\n",
       "      <td>2.061258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>7.183200</td>\n",
       "      <td>2.059105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SafeTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
    "    training_args.resume_from_checkpoint = last_checkpoint\n",
    "\n",
    "# Start training with error handling\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    # Save current state before crashing\n",
    "    print(\"Saving emergency checkpoint...\")\n",
    "    trainer.save_model(\"./emergency_checkpoint\")\n",
    "    print(\"Emergency checkpoint saved to ./emergency_checkpoint\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T23:39:35.667895Z",
     "iopub.status.busy": "2025-07-10T23:39:35.667306Z",
     "iopub.status.idle": "2025-07-10T23:39:36.058045Z",
     "shell.execute_reply": "2025-07-10T23:39:36.057330Z",
     "shell.execute_reply.started": "2025-07-10T23:39:35.667872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHqCAYAAACZcdjsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABla0lEQVR4nO3dd3wUdf7H8fdmk2waCZAEEiAQqnRUiiIieII0OQQUBDzBgqeCitj1FLAcoic/PHs7UE+wAqKCGJBiAQEFBESKRycQQUiAQLLZnd8fQxZCeibJTpLX8/GYR3ZnZ2a/k89usu+d+X7HYRiGIQAAAACwIMDfDQAAAABQ8REsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLACgCEaNGqXExMQSrTtx4kQ5HI7SbVAlldfvKjExUaNGjSp03RkzZsjhcGjnzp2l1p6dO3fK4XBoxowZpbZNAKisCBYAKjSHw1GkaenSpf5uaqWSkpKiwMBAXX/99fkuc+zYMYWGhmrQoEHl2LKSmTlzpqZNm+bvZuQwatQoRURE+LsZAFBkgf5uAABY8d577+W4/+677yopKSnX/BYtWlh6njfffFNer7dE6/7jH//QQw89ZOn57aZWrVrq2bOnPvvsM6WnpyssLCzXMrNnz9apU6cKDB9FsWXLFgUElO33YDNnztTGjRs1bty4HPMbNGigkydPKigoqEyfHwAqA4IFgArt3A+tK1euVFJSUqEfZvP7MJwfKx8sAwMDFRhY+f7cjhgxQl999ZXmzZun6667LtfjM2fOVFRUlPr162fpeVwul6X1rXA4HAoJCfHb8wNARcKpUAAqve7du6t169b66aefdNlllyksLEyPPPKIJOmzzz5Tv379VKdOHblcLjVu3FhPPvmkPB5Pjm2c28ci+9z7f/3rX3rjjTfUuHFjuVwudezYUatXr86xbl79BhwOh8aOHau5c+eqdevWcrlcatWqlb766qtc7V+6dKk6dOigkJAQNW7cWK+//nqR+m2MHTtWERERSk9Pz/XYsGHDFBcX59vPNWvWqFevXoqJiVFoaKgaNmyom266qcDtDxw4UOHh4Zo5c2aux1JSUrR48WJdc801crlc+vbbb3Xttdeqfv36crlcSkhI0D333KOTJ08W+BxS3n0sNm3apL/85S8KDQ1VvXr19NRTT+V5RKko9e3evbu+/PJL7dq1y3fqXHat8+tj8c0336hr164KDw9X9erVNWDAAG3evDnHMtk12r59u0aNGqXq1asrKipKN954Y541KamPP/5Y7du3V2hoqGJiYnT99ddr3759OZY5cOCAbrzxRtWrV08ul0vx8fEaMGBAjv4oJXkNAMDZKt9XaACQh8OHD6tPnz667rrrdP3116t27dqSzA6/ERERGj9+vCIiIvTNN9/o8ccfV1pamp577rlCtztz5kwdO3ZMf//73+VwOPTss89q0KBB+t///lfoUY7vvvtOs2fP1h133KFq1arp3//+twYPHqzdu3crOjpakrR27Vr17t1b8fHxmjRpkjwej5544gnFxsYW2rahQ4fq5Zdf1pdffqlrr73WNz89PV2ff/65Ro0aJafTqZSUFF155ZWKjY3VQw89pOrVq2vnzp2aPXt2gdsPDw/XgAED9Mknn+jPP/9UzZo1fY99+OGH8ng8GjFihCTzw296erpuv/12RUdHa9WqVXrxxRe1d+9effzxx4Xuy9kOHDigyy+/XFlZWXrooYcUHh6uN954Q6GhobmWLUp9H330UaWmpmrv3r36v//7P0kqsG/DokWL1KdPHzVq1EgTJ07UyZMn9eKLL6pLly76+eefc3XyHzJkiBo2bKjJkyfr559/1ltvvaVatWppypQpxdrvvMyYMUM33nijOnbsqMmTJ+vgwYN64YUX9P3332vt2rWqXr26JGnw4MHatGmT7rzzTiUmJiolJUVJSUnavXu3735JXgMAkIMBAJXImDFjjHP/tHXr1s2QZLz22mu5lk9PT8817+9//7sRFhZmnDp1yjdv5MiRRoMGDXz3d+zYYUgyoqOjjT///NM3/7PPPjMkGZ9//rlv3oQJE3K1SZIRHBxsbN++3Tdv/fr1hiTjxRdf9M3r37+/ERYWZuzbt883b9u2bUZgYGCubZ7L6/UadevWNQYPHpxj/kcffWRIMpYvX24YhmHMmTPHkGSsXr26wO3l5csvvzQkGa+//nqO+RdffLFRt25dw+PxGIaR9+958uTJhsPhMHbt2uWbl9fvqkGDBsbIkSN998eNG2dIMn788UffvJSUFCMqKsqQZOzYscM3v6j17devX476Zsuu8/Tp033zzj//fKNWrVrG4cOHffPWr19vBAQEGDfccEOufbnppptybHPgwIFGdHR0ruc618iRI43w8PB8H8/MzDRq1apltG7d2jh58qRv/hdffGFIMh5//HHDMAzjyJEjhiTjueeey3dbVl4DAJCNU6EAVAkul0s33nhjrvlnf8t97NgxHTp0SF27dlV6erp+++23Qrc7dOhQ1ahRw3e/a9eukqT//e9/ha7bo0cPNW7c2He/bdu2ioyM9K3r8Xi0aNEiXX311apTp45vuSZNmqhPnz6Fbt/hcOjaa6/V/Pnzdfz4cd/8Dz/8UHXr1tWll14qSb5vtb/44gu53e5Ct3u27G+5zz4daseOHVq5cqWGDRvm63R99u/5xIkTOnTokC655BIZhqG1a9cW6znnz5+viy++WJ06dfLNi42N9R0dOZvV+p4rOTlZ69at06hRo3IcoWnbtq169uyp+fPn51rntttuy3G/a9euOnz4sNLS0or9/Gdbs2aNUlJSdMcdd+ToB9KvXz81b95cX375pSTzdxAcHKylS5fqyJEjeW7LymsAALIRLABUCXXr1lVwcHCu+Zs2bdLAgQMVFRWlyMhIxcbG+jp+p6amFrrd+vXr57ifHTLy+wBX0LrZ62evm5KSopMnT6pJkya5lstrXl6GDh2qkydPat68eZKk48ePa/78+br22mt9fTS6deumwYMHa9KkSYqJidGAAQM0ffp0ZWRkFLr9wMBADR06VN9++63vvP7skHH2B/3du3f7PoxHREQoNjZW3bp1k1S03/PZdu3apaZNm+aaf9555+WaZ7W+eT13fs/VokULHTp0SCdOnMgx38prpKRtad68ue9xl8ulKVOmaMGCBapdu7Yuu+wyPfvsszpw4IBveSuvAQDIRrAAUCXkdf790aNH1a1bN61fv15PPPGEPv/8cyUlJfnOfS/K8LJOpzPP+YZhlOm6RXXxxRcrMTFRH330kSTp888/18mTJzV06FDfMg6HQ5988olWrFihsWPHat++fbrpppvUvn37HEc68nP99dfL6/Vq1qxZkqRZs2apZcuWOv/88yWZR1569uypL7/8Ug8++KDmzp2rpKQkX4fokg7jW5jSqG9pKI86F2bcuHHaunWrJk+erJCQED322GNq0aKF72iR1dcAAEgECwBV2NKlS3X48GHNmDFDd999t6666ir16NEjx6lN/lSrVi2FhIRo+/btuR7La15+hgwZoq+++kppaWn68MMPlZiYqIsvvjjXchdffLGefvpprVmzRu+//742bdqkDz74oNDtX3TRRWrcuLFmzpyp9evXa9OmTTmOVmzYsEFbt27V888/rwcffFADBgxQjx49cpzeVRwNGjTQtm3bcs3fsmVLjvvFqW9Rr4zeoEGDPJ9Lkn777TfFxMQoPDy8SNuyqqC2bNmyxfd4tsaNG+vee+/V119/rY0bNyozM1PPP/98jmVK+hoAAIlgAaAKy/4m+exvjjMzM/XKK6/4q0k5OJ1O9ejRQ3PnztX+/ft987dv364FCxYUeTtDhw5VRkaG3nnnHX311VcaMmRIjsePHDmS69vz7KMNRT0VZsSIEVq7dq0mTJggh8Oh4cOH59gPKefv2TAMvfDCC0Xeh7P17dtXK1eu1KpVq3zz/vjjD73//vs5litOfcPDw4t0alR8fLzOP/98vfPOOzp69Khv/saNG/X111+rb9++xd2dEuvQoYNq1aql1157LUedFixYoM2bN/uuH5Kenq5Tp07lWLdx48aqVq2ab73SeA0AAMPNAqiyLrnkEtWoUUMjR47UXXfdJYfDoffee69cT1EpzMSJE/X111+rS5cuuv322+XxePTSSy+pdevWWrduXZG2ceGFF6pJkyZ69NFHlZGRkeM0KEl655139Morr2jgwIFq3Lixjh07pjfffFORkZFF/qB8/fXX64knntBnn32mLl265BhytXnz5mrcuLHuu+8+7du3T5GRkfr0009L3MfggQce0HvvvafevXvr7rvv9g0326BBA/3yyy++5YpT3/bt2+vDDz/U+PHj1bFjR0VERKh///55Pv9zzz2nPn36qHPnzrr55pt9w81GRUVp4sSJJdqn/Ljdbj311FO55tesWVN33HGHpkyZohtvvFHdunXTsGHDfMPNJiYm6p577pEkbd26VVdccYWGDBmili1bKjAwUHPmzNHBgwd9FzYsjdcAADDcLIBKJb/hZlu1apXn8t9//71x8cUXG6GhoUadOnWMBx54wFi4cKEhyViyZIlvufyGm81rCE9JxoQJE3z38xtudsyYMbnWPXdoVcMwjMWLFxsXXHCBERwcbDRu3Nh46623jHvvvdcICQnJ57eQ26OPPmpIMpo0aZLrsZ9//tkYNmyYUb9+fcPlchm1atUyrrrqKmPNmjVF3r5hGEbHjh0NScYrr7yS67Fff/3V6NGjhxEREWHExMQYo0eP9g2ve/ZQrkUZbtYwDOOXX34xunXrZoSEhBh169Y1nnzySePtt9/ONdxsUet7/PhxY/jw4Ub16tUNSb5a5zXcrGEYxqJFi4wuXboYoaGhRmRkpNG/f3/j119/zbFM9r788ccfOeZPnz49VzvzMnLkSENSnlPjxo19y3344YfGBRdcYLhcLqNmzZrGiBEjjL179/oeP3TokDFmzBijefPmRnh4uBEVFWVcdNFFxkcffeRbprReAwCqNodh2OirOQBAkVx99dXatGlTnn0NAADwB/pYAIDNnTx5Msf9bdu2af78+erevbt/GgQAQB44YgEANhcfH69Ro0apUaNG2rVrl1599VVlZGRo7dq1eV7PAQAAf6DzNgDYXO/evTVr1iwdOHBALpdLnTt31j//+U9CBQDAVvx6xOLYsWN67LHHNGfOHKWkpOiCCy7QCy+8oI4dO/qrSQAAAABKwK99LG655RYlJSXpvffe04YNG3TllVeqR48e2rdvnz+bBQAAAKCY/HbE4uTJk6pWrZo+++wz30V8JHMs8T59+uQ5bjcAAAAAe/JbH4usrCx5PB6FhITkmB8aGqrvvvsuz3UyMjJyXAHU6/Xqzz//VHR0tBwOR5m2FwAAAKhqDMPQsWPHVKdOHQUEFHyyk9+CRbVq1dS5c2c9+eSTatGihWrXrq1Zs2ZpxYoVatKkSZ7rTJ48WZMmTSrnlgIAAABV2549e1SvXr0Cl/Fr5+3ff/9dN910k5YvXy6n06kLL7xQzZo1008//aTNmzfnWv7cIxapqamqX7++tm7dqpo1a5Zn01EAt9utJUuW6PLLL1dQUJC/mwNRE7uiLvZEXeyHmtgTdbGn0q7LsWPH1LBhQx09elRRUVEFLuvX4WYbN26sZcuW6cSJE0pLS1N8fLyGDh2qRo0a5bm8y+WSy+XKNb9mzZqKjo4u6+aiiNxut8LCwhQdHc0fGpugJvZEXeyJutgPNbEn6mJPpV2X7G0UpduBLa68HR4ervj4eB05ckQLFy7UgAED/N0kAAAAAMXg1yMWCxculGEYOu+887R9+3bdf//9at68uW688UZ/NgsAAABAMfn1iEVqaqrGjBmj5s2b64YbbtCll16qhQsXcjgNAAAAqGD8esRiyJAhGjJkiD+bAAAAUCF4vV5lZmb6uxk+brdbgYGBOnXqlDwej7+bg9OKW5egoCA5nc5SeW6/BgsAAAAULjMzUzt27JDX6/V3U3wMw1BcXJz27NnD9cRspCR1qV69uuLi4izXkWABAABgY4ZhKDk5WU6nUwkJCYVepKy8eL1eHT9+XBEREbZpE4pXF8MwlJ6erpSUFElSfHy8pecmWAAAANhYVlaW0tPTVadOHYWFhfm7OT7Zp2aFhIQQLGykuHUJDQ2VJKWkpKhWrVqWToviVQAAAGBj2efJBwcH+7klqKyyA6vb7ba0HYIFAABABUA/BpSV0nptESwAAAAAWEawAAAAQIWQmJioadOmFXn5pUuXyuFw6OjRo2XWJpxBsAAAAKgKvB7p4FJp5yzzp7fsrj3hcDgKnCZOnFii7a5evVq33nprkZe/5JJLlJycrKioqBI9X1ERYEyMCgUAAFDZ7Zkt/XS3lL73zLywelL7F6SEQaX+dMnJyb7bH374oR5//HFt2bLFNy8iIsJ32zAMeTweBQYW/rE0Nja2WO0IDg5WXFxcsdZByXHEAgAAoDLbM1v69pqcoUKS0veZ8/fMLvWnjIuL801RUVFyOBy++7/99puqVaumBQsWqH379nK5XPruu+/0+++/a8CAAapdu7YiIiLUsWNHLVq0KMd2zz0VyuFw6K233tLAgQMVFhampk2bat68eb7Hzz2SMGPGDFWvXl0LFy5UixYtFBERod69e+cIQllZWbrrrrtUvXp1RUdH68EHH9TIkSN19dVXl/j3ceTIEd1www2qUaOGwsLC1KdPH23bts33+K5du9S/f3/VqFFD4eHhatWqlebPn+9bd8SIEYqNjVVoaKiaNm2q6dOnl7gtZYlgAQAAUJEYhpR1omhTZpq05i5JRl4bMn+sudtcrijbM/LaTsk89NBDeuaZZ7R582a1bdtWx48fV9++fbV48WKtXbtWvXv3Vv/+/bV79+4CtzNp0iQNGTJEv/zyi/r27asRI0bozz//zHf59PR0/etf/9J7772n5cuXa/fu3brvvvt8j0+ZMkXvv/++pk+fru+//15paWmaO3eupX0dNWqU1qxZo3nz5mnFihUyDEN9+/b1De86ZswYZWRkaPny5dqwYYOmTJniO6rz2GOP6ddff9WCBQu0efNmvfrqq4qJibHUnrLCqVAAAAAViSdd+iii8OWKxJBO7pU+KWIfhCHHpcDwUnnmJ554Qj179vTdr1mzptq1a+e7/+STT2rOnDmaN2+exo4dm+92Ro0apWHDhkmS/vnPf+rf//63Vq1apd69e+e5vNvt1muvvabGjRtLksaOHasnnnjC9/iLL76ohx9+WAMHDpQkvfTSS76jByWxbds2zZs3T99//70uueQSSdL777+vhIQEzZ07V9dee612796twYMHq02bNpKkRo0a+dbfvXu3LrjgAnXo0EGSedTGrjhiAQAAgHKX/UE52/Hjx3XfffepRYsWql69uiIiIrR58+ZCj1i0bdvWdzs8PFyRkZFKSUnJd/mwsDBfqJCk+Ph43/Kpqak6ePCgOnXq5Hvc6XSqffv2xdq3s23evFmBgYG66KKLfPOio6N13nnnafPmzZKku+66S0899ZS6dOmiCRMm6JdffvEte/vtt+uDDz7Q+eefrwceeEA//PBDidtS1jhiAQAAUJE4w8wjB0WRslxa2rfw5brPl2pdVrTnLiXh4TmPfNx3331KSkrSv/71LzVp0kShoaG65pprlJmZWeB2goKCctx3OBzyer3FWt4oxVO8SuKWW25Rr1699OWXX+rrr7/W5MmT9fzzz+vOO+9Unz59tGvXLs2fP19JSUm64oorNGbMGP3rX//ya5vzwhELAACAisThME9HKsoUd6U5+pPyu7KyQwpLMJcryvbK8Orf33//vUaNGqWBAweqTZs2iouL086dO8vs+fISFRWl2rVra/Xq1b55Ho9HP//8c4m32aJFC2VlZenHH3/0zTt8+LC2bNmili1b+uYlJCTotttu0+zZs3XvvffqzTff9D0WGxurkSNH6r///a+mTZumN954o8TtKUscsQAAAKisApzmkLLfXiMzXJz9zfzpkNB+mrmcnzVt2lSzZ89W//795XA49NhjjxV45KGs3HnnnZo8ebKaNGmi5s2b68UXX9SRI0fkKEKo2rBhg6pVq+a773A41K5dOw0YMECjR4/W66+/rmrVqumhhx5S3bp1NWDAAEnSuHHj1KdPHzVr1kxHjhzRkiVL1KJFC0nS448/rvbt26tVq1bKyMjQF1984XvMbggWAAAAlVnCIKnrJ/lcx2JamVzHoiSmTp2qm266SZdccoliYmL04IMPKi0trdzb8eCDD+rAgQO64YYb5HQ6deutt6pXr15yOgsPX5ddlvN0MqfTqaysLE2fPl133323rrrqKmVmZuqyyy7T/PnzfadleTwejRkzRnv37lVkZKR69+6t//u//5NkXovj4Ycf1s6dOxUaGqquXbvqgw8+KP0dLwUOw98nlVmQlpamqKgoHTp0SNHR0f5uDk5zu92aP3+++vbtm+s8RvgHNbEn6mJP1MV+qnpNTp06pR07dqhhw4YKCQkp+Ya8HumPb6WTyVJovBTb1dKRCq/Xq7S0NEVGRiogoPKeXe/1etWiRQsNGTJETz75pL+bU6iS1KWg11j25+3U1FRFRkYWuB2OWAAAAFQFAU6pdnd/t8L2du3apa+//lrdunVTRkaGXnrpJe3YsUPDhw/3d9Nsr/LGSwAAAKCYAgICNGPGDHXs2FFdunTRhg0btGjRItv2a7ATjlhUJKV8CBMAAAA5JSQk6Pvvv/d3MyokgkVFsWd2Pp2uXrBNpysAAABUXZwKVRHsmW0OE3d2qJCk9H3m/D2z/dMuAAAA4DSChd15PeaRCuU1eNfpeT+NM5cDAAAA/IRgYXd/fJv7SEUOhpS+x1wOAAAA8BOChd2dTC7achsmSvvmS56MMm0OAAAAkBc6b9tdaHzRlktZZk5BkVLd/lLCYCm+txQYWrbtAwAAAMQRC/uL7WqO/iRHPgs4JFes1OR2M4S406Sd70vfDpI+jZG+vVba+YHkPlaerQYAALCse/fuGjdunO9+YmKipk2bVuA6DodDc+fOtfzcpbWdqoRgYXcBTnNIWUm5w8Xp+51ekzq9Il29V+r5vdR8vBTeQPKkS3s+kX4YJn0aKy0bIP3vXSnzSHnuAQAAsAGPR1q6VJo1y/zpKcNxX/r376/evXvn+di3334rh8OhX375pdjbXb16tW699Varzcth4sSJOv/883PNT05OVp8+fUr1uc41Y8YMVa9evUyfozwRLCqChEFS10+ksLo554fVM+dnX8fCESDFXiJd+Lz01x1Sr9VSy4ekak0lb4a0b560cqT0aS1pSW9p+5vSqT/Kf38AAEC5mj1bSkyULr9cGj7c/JmYaM4vCzfffLOSkpK0d2/uAWimT5+uDh06qG3btsXebmxsrMLCwkqjiYWKi4uTy+Uql+eqLAgWFUXCIOmvO6UrlkiXzDR//nVH/hfHczik6A7S+ZOlq7ZIfX+RWk+QolpLRpaUvFBadas0J05a/Bdp68tS+v5y3SUAAFD2Zs+WrrlGOvcz/r595vyyCBdXXXWVYmNjNWPGjBzzjx8/ro8//lg333yzDh8+rGHDhqlu3boKCwtTmzZtNGvWrAK3e+6pUNu2bdNll12mkJAQtWzZUklJSbnWefDBB9WsWTOFhYWpUaNGeuyxx+R2uyWZRwwmTZqk9evXy+FwyOFw+Np87qlQGzZs0F/+8heFhoYqOjpat956q44fP+57fNSoUbr66qv1r3/9S/Hx8YqOjtaYMWN8z1USu3fv1oABAxQREaHIyEgNGTJEBw8e9D2+fv16XX755apWrZoiIyPVvn17rVmzRpK0a9cu9e/fXzVq1FB4eLhatWql+fPnl7gtRUHn7YokwCnV7l789RwOqXobc2o7UUrbIu35VNr9qXTkZ+ngEnNac6cU09ns+F1/sHk6FQAAsBXDkNLTi7asxyPddZe5Tl7bcTiku++WevSQnM7CtxcWZq5TmMDAQN1www2aMWOGHn30UTlOr/Txxx/L4/Fo2LBhOn78uNq3b68HH3xQkZGR+vLLL/W3v/1NjRs3VqdOnQp9Dq/Xq0GDBql27dr68ccflZqamqM/RrZq1appxowZqlOnjjZs2KDRo0erWrVqeuCBBzR06FBt3LhRX331lRYtWiRJioqKyrWNEydOqFevXurcubNWr16tlJQU3XLLLRo7dmyO8LRkyRLFx8dryZIl2r59u4YOHarzzz9fo0ePLvyXlsf+ZYeKZcuWKSsrS2PGjNHQoUO1dOlSSdKIESN0wQUX6NVXX5XT6dS6desUFBQkSRo7dqzcbreWL1+u8PBw/frrr4qIiCh2O4qDYFEVRZ4ntXrEnI7vMK/cvedT6dAK6dAP5rT2XqlmBzNkJAyWIpv6u9VVh9djXpfkZLLZIT+2qxkqUbl4PXKkLFPdrOVypIRL8ZdTZwBFkp4uldbnQ8Mwj2Tk8Vk6T8ePS+HhRVv2pptu0nPPPadly5ape/fukszToAYPHqyoqChFRUXpvvvu8y1/5513auHChfroo4+KFCwWLVqk3377TQsXLlSdOnUkSf/85z9z9Yv4xz/+4budmJio++67Tx988IEeeOABhYaGKiIiQoGBgYqLi8v3uWbOnKlTp07p3XffVfjpX8BLL72k/v37a8qUKapdu7YkqUaNGnrppZfkdDrVvHlz9evXT4sXLy5RsFi8eLE2bNigHTt2KCEhQZL07rvvqlWrVlq9erU6duyo3bt36/7771fz5s0lSU2bNpXX61VaWpr27NmjwYMHq02bNpKkRo0aFbsNxcWpUFVdREOpxb3SlT9IV++R2v9bqtXN7K/x5xpp/cPSF82k+W2lDZOko5vy/toDpWPPbGleorT4cumH4ebPeYnm/MrM65EOLpV2zjJ/VvYryZ+uc+CynuqQMVWBy3pWjToDqFKaN2+uSy65RP/5z38kSdu3b9e3336rm2++WZLk8Xj05JNPqk2bNqpZs6YiIiK0cOFC7d69u0jb37x5sxISEnyhQpI6d+6ca7kPP/xQXbp0UVxcnCIiIvSPf/yjyM9x9nO1a9fOFyokqUuXLvJ6vdqyZYtvXqtWreQ869BPfHy8UlJSivVcZz9nQkKCL1RIUsuWLVW9enVt3rxZkjR+/Hjdcsst6tGjh5555hn9/vvvvmXHjh2rp556Sl26dNGECRNK1Fm+uPwaLDwejx577DE1bNhQoaGhaty4sZ588kkZfHD1j7B60nl3Sj2WSgOTpU6vS3FXSo5A6egG8yJ881tLX7aQ1j8q/fkzIaM07ZktfXtN7iutp+8z51fWD51VLUxV1ToDKDVhYeaRg6JMRT2lfv78om2vuP2mb775Zn366ac6duyYpk+frsaNG6tbt26SpOeee04vvPCCHnzwQS1ZskTr1q1Tr169lJmZWczfSP5WrFihESNGqG/fvvriiy+0du1aPfroo6X6HGfLPg0pm8PhkNfrLZPnkswRrTZt2qR+/frpm2++UcuWLTVnzhxJ0i233KL//e9/+tvf/qYNGzaoQ4cOevHFF8usLZKfg8WUKVP06quv6qWXXtLmzZs1ZcoUPfvss2W+0yiCkFpSk1ulvyyUBh2ULp4u1blKCgg2+2hs+qf0VXtpXiPp5/ukP1ZIRtm9cSo9r0f66W5JeQW10/PW3CllHJay0iVvVuUIdVXtQ3ZR6vzTuMp/xAaAJQ6HeTpSUaYrr5Tq1cu/X4TDISUkmMsVZXtF6V9xtiFDhiggIEAzZ87Uu+++q5tuusnX3+L777/XgAEDdP3116tdu3Zq1KiRtm7dWuRtt2jRQnv27FFycrJv3sqVK3Ms88MPP6hBgwZ69NFH1aFDBzVt2lS7du3KsUxwcLA8hYy926JFC61fv14nTpzwzfv+++8VEBCg8847r8htLo7s/duzZ49v3q+//qqjR4+qZcuWvnnNmjXTPffco6+//lqDBg3K0ecjISFBt912m2bPnq17771Xb775Zpm0NZtf+1j88MMPGjBggPr16yfJPO9t1qxZWrVqlT+bhXO5akqNRpmTO03a96XZJ2P/fOnETum3580ptK6UMEiOOgMkgw9G+fJ6pJP7zd/diZ1mP5dDK3J/uM7BMNf5NOaseQ4z6AUES87gM7dzTC7JGSyngnTRqaNyfv8fKTAk93L5rp/9uKuAxwpZNyAo7/9EhX7IdpgfsusOKLjvgWGYI5153Wd+Zk+G2wxhvtvufJbLOmedPJY5e15Jt5XxZ+F1Tt9j9rEpyUANAHAOp1N64QVz9CeHI+d3Utl/mqdNK1rH7ZKIiIjQ0KFD9fDDDystLU2jRo3yPda0aVN98skn+uGHH1SjRg1NnTpVBw8ezPGhuSA9evRQs2bNNHLkSD333HNKS0vTo48+mmOZpk2bavfu3frggw/UsWNHffnll75v9LMlJiZqx44dWrdunerVq6dq1arlGmZ2xIgRmjBhgkaOHKmJEyfqjz/+0J133qm//e1vvv4VJeXxeLRu3boc81wul3r06KE2bdpoxIgRmjZtmrKysnTHHXeoW7du6tChg06ePKn7779f11xzjRo2bKi9e/dq9erVGjTIHDH0nnvuUd++fdWsWTMdOXJES5YsUYsWLSy1tTB+DRaXXHKJ3njjDW3dulXNmjXT+vXr9d1332nq1Kl5Lp+RkaGMjAzf/bS0NEmS2+22NJQXiiNUqnuNOWWdkOPAQgXsnSNH8nw5Tu6Ttr6owK0vqpeipNWDlZUwWEat7uaHy6rC8EqnDshxYpd0Yocc6btO395p/kzfLYdRGq9Xw7w+iTdDyip4yQBJcZK0/6dSeN7iMxxBuUOKN0uOUwUNcWx+yDbm1jeXP/dD/en7DqOQna+Aso7vkVGTv2n+kv3/hP8r9lHVa+J2u2UYhrxeb4lOq7n6aumjj6R77nFo794zX/TUq2do6lRDV18tleRsnexT17Pblp8bb7xRb7/9tvr06aO4uDjfso888oh+//139erVS2FhYRo9erQGDBig1NTUHNs7d/tn3//00081evRoderUyTcUbd++fX2/q6uuukrjxo3T2LFjlZGRob59++of//iHJk2a5NvGwIED9emnn+ryyy/X0aNH9fbbb/sCUPZ2QkJCtGDBAt1zzz3q2LGjwsLCNGjQID3//PO+7RiGkWdbs7eTF6/Xq+PHj+uCCy7IMb9x48baunWr5syZo7vuukuXXXaZAgIC1KtXL/373/+W1+uVw+HQoUOHdMMNN+jgwYOKiYnRwIEDNWHCBLndbnk8Ho0ZM0Z79+5VZGSkevXqpalTp+bZFq/XK8Mw5Ha7c/QRkYr3vnMYfuzQ4PV69cgjj+jZZ5+V0+mUx+PR008/rYcffjjP5SdOnKhJkyblmj9z5sxyu1gK8hZgZCrWs151PCsUl7VKwTozrnOmInQgsJP2OzvrD+f58joqeMgwDLmMowozUszJe/Cs2ykKNVLkLOSTvldOnXTEKN1RS+kBtSRDauBZXOhTf++aqCPO8xSgLAUYWebP05PDcOe4f+Zxt+++45z7vnXPXf6cx5yG+/S6eW/bcdbyTtnjH79XgfLKKUNOeRUow+HMeV9OeR2nfxZpuQAZp7eZvd6ZdQPldZz+edZyhuPsNjhVzbtXrdzvFtr2o45EbQu+VsnOi2Q4GLwPqOqyRyxKSEhQcHBwibfj8UgrVgTqwAGH4uIMde6cVWZHKlCxZGZmas+ePTpw4ICysnJ+hklPT9fw4cOVmpqqyMjIArfj12DxwQcf6P7779dzzz2nVq1aad26dRo3bpymTp2qkSNH5lo+ryMWCQkJSk5OVnR0dHk2HQVwZ6Rr3cL/U8f4PQpM/lyOjDNX9zYCq8mI7ytvvYEy4npLgTYMhIYhZR6S4/SpSuZRhl1ynNh5et4uObynCt6EAqSwBBnhDaTwRBlhDU7fbmj+DKkjBZz1gdHwKPDLJtLJ/XLkcWqQIYcUWldZ/bZJjuL/F3C73UpKSlLPnj1zdSwrdYZhngrnzcxjypC8mXJ43XIc/lHOdfcUujlPu6kyojvKCAgy9z0gSHIEmT8DgszBBXLNs+l/ykLrbMr+PtEIiZO34U3yNrrFHFwB5aJc3y8okqpek1OnTmnPnj1KTExUSEiIv5vjYxiGjh07pmrVqvn6TcD/SlKXU6dOaefOnUpISMj1GktLS1NMTEyRgoVfvwq7//779dBDD+m6666TJLVp00a7du3S5MmT8wwWLpcrz0urBwUFVck/NPYVpj8CL5A6PiqH8w3pj+/MPhl7PpXj5H459nyogD0fSs5QqU5f8zoZdftJQXm8WMvimg6GIWX+ebp/w07pxI7TP8+67SnsykMO84NeeKI5RTQ8/TNRCm8oR1hdKSBIRf8zGyR1+LfZYVkO5ex34DC30+EFBQVb+4dSvu+V0IIfju0kbX3e7KidZz8L83fsbHFXJbq+QxHq3PE16eReafubcpw6IOfmf8r52zNS3b9KTW+X4nqYw0GjzPG/xX6qak08Ho8cDocCAgIUEGCf93/2KTXZbYM9lKQuAQEBcjgceb7HivOe82uwSE9Pz7XDTqezTIflQjkLcEq1u5lT+2nSoR99IUMndp65HRBsDm1bf7D5AcpV0xwR6Ke7c3Z2DasntX9BShhU8PNmHjU7Rfs6SO80Q0P27axjhbc9tM6ZwHBugAhLMDstl6aEQVLXT/LZ52mF73NFE+A0a5nPh2xJ5n5XmlBxWlHr3Poxae9caesrUspS8/beuVJEE6npbeZgCi6O1AIA7MOvwaJ///56+umnVb9+fbVq1Upr167V1KlTddNNN/mzWSgrjgAptrM5XfCcdGStGSp2fyId2yrt/8KcHIFSVEvpaB4XcskehrTzu1L1tmdGVTp7hKUTOyV3auHtCYnLcZTB/Hn6dniC5PTD4eaEQeYoSFXlyttVLUxlO13nrOQlWrdygc6/uI8Cz73ydkCQVP9ac0r9Vdr2mrTjHen4dmntfdIv/5DqDzWPYkR3Kv4YkCgfZXHUFQBsyq/B4sUXX9Rjjz2mO+64QykpKapTp47+/ve/6/HHH/dns1AeHA6p5oXm1PYpKXXTmaMXRzfkHSok+b7VXvG3wp/DFZvziEOOow8NpMBCTtXxlwBn1RpqtKqFqWwBThm1umlf4Am1q9Wt4P2NammeQnX+ZGnnTGnbK9KRdWbQ2PGOVONCM2AkDpMCw/PfDsqXlaOuAFAB+TVYVKtWTdOmTdO0adP82Qz4m8MhVW9tTm0mSP/7r7SyCMEhKFKq1jR3aIhoeDo48AGrwqhqYaqkAsOlJqOlxrdIh3+Utr0q7fpQOvKztGq0eSSj4UgzZEQ193drq7bsiz+e238o+6hr108IFyg2P463g0qutLohMI4h7Keo31R3fM38hhaoahwOKeZic7pwqvS/6WbIOP4/aeu/zan25VLTO6R6A6rWdWTsoLQu/gicFhQUJIfDoT/++EOxsbG2GYHJ6/UqMzNTp06dovO2jRSnLoZhKDMzU3/88YcCAgIsDWcsESxgR6HxpbscUJm5oqUW90nNx0vJSeZpUvu/kA4uMafQePMIR5NbGbK2vBxcyhXWUaqcTqfq1aunvXv3aufOnf5ujo9hGDp58qRCQ0NtE3ZQsrqEhYWpfv36lgMiwQL2E9vV/ABUyDCkiu1a3i0D7MsRINXpZU4ndkvb35B+f8vst7LxSWnTP6W6/c2jGHFXMGRtafJ6zH5hKUvNUHFgUdHWS/lWqtWNjvcokoiICDVt2tRWVx93u91avny5Lrvssio5DLBdFbcuTqdTgYGBpRIOCRawn6o6DClQWsLrS+2eklo/bg5Ru+0VKWXZOUPW3n56yNqa/m1rRWR4zSBxcKkZJlKWS5lHir+dDY+bne8Th0sNhklRLUq7pahknE6nnDa6VLbT6VRWVpZCQkIIFjbiz7oQLGBPVXUYUqA0OYOlBkPM6egmaftr0o53Tw9Ze6/0y6NSg+ukJrdL0R355jw/htccre7gkvyDRGCEeRS19uVSra7St9dKJ/M76irJGWZerPP47+YRpY1PSjXOlxoMN2sSnlDGOwUApY9gAfuqqsOQAmWheiupw4tSu8nSrplmZ+8j66T/zTCnGhdKze4wvzkPDPNzY/3M8CrSs0MB216UDn1rHu3JN0h0l2p1N4fODjjrX2qHQo66XvKeeVHQffPMIYSTF5r1OLJOWveAVOsysxYJ10ghMWW4swBQeggWsDeGIQVKV1CE2ZG78Wjp0EozYOw+PWTtj7dIP99rniLV5LaqM2St4ZWObvT1kQhMWabLM/+U1p21TGD4OUGifc4gca6iHnVNHG5OGYfNi4XummkeEcme1twpxfcyQ0a9AWb9AMCmCBYAUBU5HFJsZ3PKHrJ2+2vmkLVbXjCn2n8x+2JUtiFrDa95Uc6DS073k1gmZf7pe9ghKUshCojrpoC4y886IlHM30Fxjrq6oqWmfzenE3vMsLdzpnRkrbT/S3Nyhpq1aDDcDBtOa8NCAkBpI1gAQFUXEiO1vF9qca+U/LV5FGP/F9LBb8wpNN48wtFkdMUcstYXJJae7iOxzDxCcLbAcCn2UqlWd2VFX6r5Kw+qT9e/KsBqx8eSHHUNTzCHEG5xn5T6m7Rrlhkyjm+Xdn1gTsE1zNOkEoebp00xyhcAGyBYAABMjgCpTm9z8g1Z++bpIWufkDY9LdX9q3kUw85D1hpeKfXXc4LEoZzLOMPMIJF9alN0B98RCcPtluGYX86NzkdUc6ntJKnNROnPNdLOWdLuD8ya/P6mOYXWlRoMNUNGjQvphA/AbwgWAIDccgxZO+f0kLXLzdt750jVmpr9MOwwZK1hmEEiZenpkZuKFyQqBIfDHLkruqN0wXPmPu6aKe3+1Bx96rep5lStmdkfI3G4FNnM360GUMUQLAAA+XMGm9+GNxhqDlm77VVzyNpj23IOWdv0DvNDb3nIESSWng4Sf5zT7jAptstZna07VJ4+CQFOKe4v5tThZSn5K/NUqX2fS8e2ShsnmVPN9qeHrx0qhdX1d6sBVAEECwBA0VRvJXV8STr/GfPb8q2vSEfXnxmytmZ7M2A0uC7vIWu9npINH20YUtrmM6c2HVyaR5AINYNEre5mmKjZsfIEiYI4XWaH7noDJPcxae9nZsg48LX050/mtPY+8wrficOlhMH+P8JUGZX0tQ1UMgQLAEDx5Bqy9hVp90fmh9gfbz4zZG3T26TI88x19szOZ+jVF3Jf8NIwpLTfcp7adCol5zJVNUgUJKia1PB6czr1h7T7Y7Pj9x/fne5rslRaM0aK720eyajX3+y0DmuK89oGKjmCBQCgZPIasnbba9KJHdKWaeZU+wqpxgXSb88r11Wo0/eZF5G79GMpquVZpzYtzSNIhEgxZ53aFN3R/LYeeQuJNS942OwO6cQucySpnbPMI0z7PjenwHCp3tVmn4z4KytWnxO72DP79IUQ83ltd/2EcIEqhWABALAuJFZq+YA5RGryQrMvxr4vpIOLzSlPpz+MfTdEkjfnQ84QKeaS00ckLidIWBHeQGr5oDkd3XRm+NoTO6Sd75uTK1pKuNY8XSq2i31H/LITr8c8UnFuqJBOz3NIP40zr2XCaVGoIggWAIDS4wiQ6vQxpxO7pPWPmh9cC+Q1vy2P7Xrm1KboTgSJslC9lVT9Kantk9LhVWbA2P2hdOqgeYHE7a9JYQlmP5nE4VL1dgxfm82bZf6eTu43p4NLc57+lIshpe+Rfvs/qW4/8/SooGrl1VrALwgWAICyEd5AqtOvCMFCUqc3pUYjy75NMDkcUsxF5nTh82Zfll2zpD2fmh+GNz9nTpHNzf4YicOkak383eqyYXjNPinZgeHsKf2s26cOKu+jE4VYd785SVJQpBkwQuuZP/OagqoT5lBhESwAAGUnNL5oy4U3KNt2IH8BgVJ8T3Pq+Iq0f77ZH2Pf52Yn+g2Pm1N0p9PD1w4puK52GSHJMMwrrOcVGE7uN9uX/dPwFG2bDqe5T6F1JEeQdOj7wtcJT5Qyj0ruo5I7zRwqOfXX/Jd3hp0TNhJy3g+tZ566RviADREsAABlJ7ar+WEofZ/y/rbXYT4e27W8W4a8OEPMzsYJg6TMVGnvXPN0qYOLzFOnDq+S1o6Xal1+evjaQVJw9TPrl8cISYYhuVPzPqpwbnDwZhZxow4ppLYZGELrSGF1ztw+e3LFnAlJXo80L7Hw13b/7eY67uPmxQzT9+aeTp7+mXFI8qSb1yM5tjX/5ga4ch/pOPcoSEitsu0r4/XIkbJMdbOWy5ESLsVfTl8SECwAAGUowGl+qPz2GkkO5fwAdvob1/bT+EBiR8FR5ulpjUZKJw+eHr52pnRoxZlO+atvl+r0NUOGN0v6YYQsjZDkPp7n6UjO9L3qcnKTAuffK53aL3lOFn0/XDG5A8K5wSGktnnkpjiK+9oOipCCzjszBHNesk7mHT5OnnX71EHJmyEd/92c8m1f0Ol9PR06whPyCB9xJXvvnQ6Qgel71UGSlk1liF1IIlgAAMpawiDzQ2We32RP44NIRRBaWzpvrDkd33F6+Nr3pdRN5lGNvXOV+8N1ttMjJK0eKwVH5+wAfW5/hqxjeT59gKQYSTpx1syg6vkfWcgODyFxZTsIQGm/tgNDzb4sBfVn8WSe/n3lETp84SNZ8rrNARRO7Mp/W75Tuwro8xFaJ+dQxAyxiwIQLAAAZS9hkDnsph3OvYc1EQ2lVg+b09EN5qlS/5t+unNzfgzzw+7i7oVvPzBcCq2bIyB4gmvr598O6oLOfRVYrb75+snr6u7+UN6vbWewFJFoTvnxZkmnDkgn9uQdPNL3muHEyDpz/3B+Gzt9mlj2kY+Di8QQu8gPwQIAUD4CnOZQsqg8qreRzp8sRbWRVowofHlXLSmyWcGnJuUxJKvX7db+7fN1fuylUpANL+Rnt9d2QOCZIw758XqkjJSC+3yk7zX7qZw6YE5aU8gTnx5id0lvqeYFZ9U4/sxPuwRClAmCBQAAsCasTtGWu/RDe30Ar8oCsk+DijcvQJkXwzA7lKfvNQPDnrnSjumFb/vgotNHNvIQFJU7bBBAKg2CBQAAsIbRvyonh0MKiTWnmheY1+EoSrBo/HczGJw7tK8n3RzRy50qpW0ueBu5AkheYaScA4hdhlK2MYIFAACwhtG/qoaiBsiOL+eutWGYnfPTzwkb515TxK4BpDyGUq4ECBYAAMA6Rv+q/KwESIfDPOIRFSlFNc//OQzDvJDguWEj1+3Tww6XRwBhJKwiI1gAAIDSwehflV9ZB0iHw7yGSnBUMQNIASHESgAJiZN+f1OMhFU0BAsAAFB67DZCEkrf6QCZlbxE61Yu0PkX91FgeV95u9gBJI9TrqwEkDNPYHZs/+NbXvciWAAAAKC4ApwyanXTvsATalerm32/rc8RQFrkv1yeAWS/dHCJlPxV4c9zMrn02lyBESwAAABQteUXQKI7FS1YhMaXXdsqkAB/NwAAAACwpeyRsLI7p+fikMISGEr5NIIFAAAAkJfskbAk5Q4XDKV8Lr8Gi8TERDkcjlzTmDFj/NksAAAAwJQ9ElZY3Zzzw+ox1Ow5/NrHYvXq1fJ4PL77GzduVM+ePXXttdf6sVUAAADAWRhKuUj8GixiY2Nz3H/mmWfUuHFjdevWzU8tAgAAAPLAUMqFss2oUJmZmfrvf/+r8ePHy+HIu4NMRkaGMjIyfPfT0tIkSW63W263u1zaicJl14Ka2Ac1sSfqYk/UxX6oiT1RF3sq7boUZzsOwzDyupRgufvoo480fPhw7d69W3Xq1MlzmYkTJ2rSpEm55s+cOVNhYXlcgh0AAABAiaWnp2v48OFKTU1VZGRkgcvaJlj06tVLwcHB+vzzz/NdJq8jFgkJCUpOTlZ0dHR5NBNF4Ha7lZSUpJ49eyooKMjfzYGoiV1RF3uiLvZDTeyJuthTadclLS1NMTExRQoWtjgVateuXVq0aJFmz55d4HIul0sulyvX/KCgIF7QNkRd7Iea2BN1sSfqYj/UxJ6oiz2VVl2Ksw1bXMdi+vTpqlWrlvr16+fvpgAAAAAoAb8HC6/Xq+nTp2vkyJEKDLTFARQAAAAAxeT3YLFo0SLt3r1bN910k7+bAgAAAKCE/H6I4Morr5RN+o8DAAAAKCG/H7EAAAAAUPERLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWOb3YLFv3z5df/31io6OVmhoqNq0aaM1a9b4u1kAAAAAiiHQn09+5MgRdenSRZdffrkWLFig2NhYbdu2TTVq1PBnswAAAAAUk1+DxZQpU5SQkKDp06f75jVs2NCPLQIAAABQEn4NFvPmzVOvXr107bXXatmyZapbt67uuOMOjR49Os/lMzIylJGR4buflpYmSXK73XK73eXSZhQuuxbUxD6oiT1RF3uiLvZDTeyJuthTadelONtxGIZhlMqzlkBISIgkafz48br22mu1evVq3X333Xrttdc0cuTIXMtPnDhRkyZNyjV/5syZCgsLK/P2AgAAAFVJenq6hg8frtTUVEVGRha4rF+DRXBwsDp06KAffvjBN++uu+7S6tWrtWLFilzL53XEIiEhQcnJyYqOji6XNqNwbrdbSUlJ6tmzp4KCgvzdHIia2BV1sSfqYj/UxJ6oiz2Vdl3S0tIUExNTpGDh11Oh4uPj1bJlyxzzWrRooU8//TTP5V0ul1wuV675QUFBvKBtiLrYDzWxJ+piT9TFfqiJPVEXeyqtuhRnG34dbrZLly7asmVLjnlbt25VgwYN/NQiAAAAACXh12Bxzz33aOXKlfrnP/+p7du3a+bMmXrjjTc0ZswYfzYLAAAAQDH5NVh07NhRc+bM0axZs9S6dWs9+eSTmjZtmkaMGOHPZgEAAAAoJr/2sZCkq666SldddZW/mwEAAADAAr8esQAAAABQORAsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABY5tdgMXHiRDkcjhxT8+bN/dkkAAAAACUQ6O8GtGrVSosWLfLdDwz0e5MAAAAAFJPfP8UHBgYqLi7O380AAAAAYIHfg8W2bdtUp04dhYSEqHPnzpo8ebLq16+f57IZGRnKyMjw3U9LS5Mkud1uud3ucmkvCpddC2piH9TEnqiLPVEX+6Em9kRd7Km061Kc7TgMwzBK5VlLYMGCBTp+/LjOO+88JScna9KkSdq3b582btyoatWq5Vp+4sSJmjRpUq75M2fOVFhYWHk0GQAAAKgy0tPTNXz4cKWmpioyMrLAZf0aLM519OhRNWjQQFOnTtXNN9+c6/G8jlgkJCQoOTlZ0dHR5dlUFMDtdispKUk9e/ZUUFCQv5sDURO7oi72RF3sh5rYE3Wxp9KuS1pammJiYooULPx+KtTZqlevrmbNmmn79u15Pu5yueRyuXLNDwoK4gVtQ9TFfqiJPVEXe6Iu9kNN7Im62FNp1aU427DVdSyOHz+u33//XfHx8f5uCgAAAIBi8GuwuO+++7Rs2TLt3LlTP/zwgwYOHCin06lhw4b5s1kAAAAAismvp0Lt3btXw4YN0+HDhxUbG6tLL71UK1euVGxsrD+bBQAAAKCY/BosPvjgA38+PQAAAIBSYqs+FgAAAAAqJoIFAAAAAMsIFgAAAAAsI1gAAAAAsKxEwWLPnj3au3ev7/6qVas0btw4vfHGG6XWMAAAAAAVR4mCxfDhw7VkyRJJ0oEDB9SzZ0+tWrVKjz76qJ544olSbSAAAAAA+ytRsNi4caM6deokSfroo4/UunVr/fDDD3r//fc1Y8aM0mwfAAAAgAqgRMHC7XbL5XJJkhYtWqS//vWvkqTmzZsrOTm59FoHAAAAoEIoUbBo1aqVXnvtNX377bdKSkpS7969JUn79+9XdHR0qTYQAAAAgP2VKFhMmTJFr7/+urp3765hw4apXbt2kqR58+b5TpECAAAAUHUElmSl7t2769ChQ0pLS1ONGjV882+99VaFhYWVWuMAAAAAVAwlOmJx8uRJZWRk+ELFrl27NG3aNG3ZskW1atUq1QYCAAAAsL8SBYsBAwbo3XfflSQdPXpUF110kZ5//nldffXVevXVV0u1gQAAAADsr0TB4ueff1bXrl0lSZ988olq166tXbt26d1339W///3vUm0gAAAAAPsrUbBIT09XtWrVJElff/21Bg0apICAAF188cXatWtXqTYQAAAAgP2VKFg0adJEc+fO1Z49e7Rw4UJdeeWVkqSUlBRFRkaWagMBAAAA2F+JgsXjjz+u++67T4mJierUqZM6d+4syTx6ccEFF5RqAwEAAADYX4mGm73mmmt06aWXKjk52XcNC0m64oorNHDgwFJrHAAAAICKoUTBQpLi4uIUFxenvXv3SpLq1avHxfEAAACAKqpEp0J5vV498cQTioqKUoMGDdSgQQNVr15dTz75pLxeb2m3EQAAAIDNleiIxaOPPqq3335bzzzzjLp06SJJ+u677zRx4kSdOnVKTz/9dKk2EgAAAIC9lShYvPPOO3rrrbf017/+1Tevbdu2qlu3ru644w6CBQAAAFDFlOhUqD///FPNmzfPNb958+b6888/LTcKAAAAQMVSomDRrl07vfTSS7nmv/TSS2rbtq3lRgEAAACoWEp0KtSzzz6rfv36adGiRb5rWKxYsUJ79uzR/PnzS7WBAAAAAOyvREcsunXrpq1bt2rgwIE6evSojh49qkGDBmnTpk167733SruNAAAAAGyuxNexqFOnTq5O2uvXr9fbb7+tN954w3LDAAAAAFQcJTpiAQAAAABnI1gAAAAAsIxgAQAAAMCyYvWxGDRoUIGPHz161EpbAAAAAFRQxQoWUVFRhT5+ww03WGoQAAAAgIqnWMFi+vTpZdUOAAAAABWYbfpYPPPMM3I4HBo3bpy/mwIAAACgmGwRLFavXq3XX39dbdu29XdTAAAAAJSA34PF8ePHNWLECL355puqUaOGv5sDAAAAoAT8HizGjBmjfv36qUePHv5uCgAAAIASKlbn7dL2wQcf6Oeff9bq1auLtHxGRoYyMjJ899PS0iRJbrdbbre7TNqI4suuBTWxD2piT9TFnqiL/VATe6Iu9lTadSnOdhyGYRil8qzFtGfPHnXo0EFJSUm+vhXdu3fX+eefr2nTpuW5zsSJEzVp0qRc82fOnKmwsLCybC4AAABQ5aSnp2v48OFKTU1VZGRkgcv6LVjMnTtXAwcOlNPp9M3zeDxyOBwKCAhQRkZGjsekvI9YJCQkKDk5WdHR0eXWdhTM7XYrKSlJPXv2VFBQkL+bA1ETu6Iu9kRd7Iea2BN1safSrktaWppiYmKKFCz8dirUFVdcoQ0bNuSYd+ONN6p58+Z68MEHc4UKSXK5XHK5XLnmBwUF8YK2IepiP9TEnqiLPVEX+6Em9kRd7Km06lKcbfgtWFSrVk2tW7fOMS88PFzR0dG55gMAAACwN7+PCgUAAACg4vPrqFDnWrp0qb+bAAAAAKAEOGIBAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAy/waLF599VW1bdtWkZGRioyMVOfOnbVgwQJ/NgkAAABACfg1WNSrV0/PPPOMfvrpJ61Zs0Z/+ctfNGDAAG3atMmfzQIAAABQTIH+fPL+/fvnuP/000/r1Vdf1cqVK9WqVSs/tQoAAABAcfk1WJzN4/Ho448/1okTJ9S5c2d/NwcAAABAMfg9WGzYsEGdO3fWqVOnFBERoTlz5qhly5Z5LpuRkaGMjAzf/bS0NEmS2+2W2+0ul/aicNm1oCb2QU3sibrYE3WxH2piT9TFnkq7LsXZjsMwDKNUnrWEMjMztXv3bqWmpuqTTz7RW2+9pWXLluUZLiZOnKhJkyblmj9z5kyFhYWVR3MBAACAKiM9PV3Dhw9XamqqIiMjC1zW78HiXD169FDjxo31+uuv53osryMWCQkJSk5OVnR0dHk2EwVwu91KSkpSz549FRQU5O/mQNTErqiLPVEX+6Em9kRd7Km065KWlqaYmJgiBQu/nwp1Lq/XmyM8nM3lcsnlcuWaHxQUxAvahqiL/VATe6Iu9kRd7Iea2BN1safSqktxtuHXYPHwww+rT58+ql+/vo4dO6aZM2dq6dKlWrhwoT+bBQAAAKCY/BosUlJSdMMNNyg5OVlRUVFq27atFi5cqJ49e/qzWQAAAACKya/B4u233/bn0wMAAAAoJX698jYAAACAyoFgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwL9HcDUHQej/Ttt1JyshQfL3XtKjmd/m4VAAAAQLCoMGbPlu6+W9q798y8evWkF16QBg3yX7vKWlUMU+xz1dhnAAAqG06FqgBmz5auuSZnqJCkffvM+bNn+6ddZW32bCkxUbr8cmn4cPNnYmLl3V+Jfa4q+yyZYWrZMoeWL6+rZcsc8nj83aKy5/FIS5dKs2aZP6vCPgNAVcIRC5vzeMwjFYaR+7HsebfcIqWlSS6XFBwsBQWZ09m3z72f322nU3I4yncf85Idps7d7+ww9cknle9IDft8RmXeZ+nsI5CBkjpo6tTKfwSSo65V52gc+1x19jn7y5HwcIcuv7xq7HNVq3NxESxs7ttvcx+pONeRI9KNN5becxYlgBQUWgIDnTp48AJ9/rlTISHFDzpOp/T3vxccpm6/XYqJkQICzHnZ87Nvl9W8snoej0d66KGC93n0aOnECSkw0Nxvp9P8WZTbXq9Dv/1WQ7GxDgUHF3/9oi5XnFBaWGh2OKRx46QBAyrXH+6qGKaq4j5LVTNMsc+mqrPPfDlSmfe5JByGkde/9YohLS1NUVFROnTokKKjo/3dnDIxa5Z5ekhh2rY1P2hnZkputzkV5bbXW/b7gKqlqMEkK0s6fLjw7TVrJkVFmetmr5/X7YIeK8rt8lhHkq6/Xvrjj7z31eGQateWFi06E7JLsk92OOqYzeMxT23L7wsSh8P857xjh30CpNvt1vz589W3b18FBQWVaBv5hans2lTGMFWW+1waNSkL1PkM9rn825Wf0n6/ZH/eTk1NVWRkZIHL+jVYTJ48WbNnz9Zvv/2m0NBQXXLJJZoyZYrOO++8Iq1fFYLF0qXmOeeFWbJE6t69+Nv3eM6EjKKGkcJunzrl0caNv6lhw+byep3FXj85Wdq+vfC2164tVatmvrHPnqTc8/Kbb5dl9++Xfv658H1u3drcb4/HDIVeb1FvGzp2LF2hoWHyeh3FXp8AWrE4HNaD1tlB0Mo2/vhD+vrrwts8eLBUv77Z9uxwlNdU0GNWH89+zOv1aOPGDWrbtrUCAwOLta7DYX74GDOm4OAcEyO9/bZ5BLKgvxmFPWb18dLattcrXXqp+fc7v9dkfLy0alXOU27P/ZnXPIfD/KC0aFGSevbsqeDgoAKXLehnaa7j8UgNG1as0GxVRfyiwKqKuM9VNlj07t1b1113nTp27KisrCw98sgj2rhxo3799VeFh4cXun5VCBbZL+h9+/I+ZaQyvqDLOkzZUVnvc2n8kTk7ZJQkmJz72KpV5uldhXnmGalVqzPrezxnprPvF/d2ea1z9u3Dhws/tVGSwsPND5x5bYuQB1QsZ4ew7Pt5LVOc+/5aJytLOnky9zLniogwj7qevX5RAltZL1+SbaWnF+3vtp0+k/gzWPi1j8VXX32V4/6MGTNUq1Yt/fTTT7rsssv81Cp7cTrN8/euuebMN2HZsl/406bZJ1SUhq5dzbBUWJjq2rX821ZWKsI+Z5/CVFpatZImTSp8n++7r/K8vosaIL/4Iv9/UNl9c0oj6JT0dnGW3bpVev31wvd52DDziIVhmOvm1b+poMdKsk5+j3k8Xh04cFCxsbXlcAQUe3sHD0pbthS+zw0bSjVr5r+ts+ttpQ9YWT129vysLPOIc2HO/hBXcU/ELp6zf19VxfHj/m5B+cvvaF1VY6vO26mpqZKkmjVr+rkl9jJokHn+Xl6dhqZNs9d5faWhKoYp9rlq7HNpBMizT72x0anm+fJ4pC+/LHyf33vPPrV2uz2aP3/V6W/7ip+mixog//Mf+3zDaVVR9/mbbwre57MDy9k/MzPdWrBggXr37uP7Bja/ZfP7WdrrfPedNHBg/vuS7dNPpUsuyb3dinh/5Urpb39Tod55R+rUqWi/T7vPW7tWuusuFSo+vvBlqgLbBAuv16tx48apS5cuat26dZ7LZGRkKCMjw3c/LS1NknnIx12Ur0oqsP79pb59pe++c/iGObv0UkNOZ9G+JSpP2bWwUpP+/aUPPnBo/Hin9u078xVX3bqGnn/eo/79Ddvtt1Vluc+lUZOyUBXr/PzzDl13nfN0mDqzzw6H+V/sX//yyOs1KtUpTxVtn62+Xy6+WKpbN1D79+fc32wOh6G6daWLL86qNK/vst5nw3DL6TQknVk5r1NZylPv3kXb5759s2wTmq2qV0966KHC93nIkMqzzx06SFOmVKz3c2n/zy/OdmwzKtTtt9+uBQsW6LvvvlO9evXyXGbixImaNGlSrvkzZ85UWFhYWTcRfuDxSL/+Gq0jR0JUo8YptWx5uNL8scoP+1z593nFini99VYbHT4c6psXE5Oum2/eqM6dK+fx9Kq2zytWxGvKlI6n7539YcT8l/vgg6sr3X6zz+yzxD5XRunp6Ro+fLj9O29nGzt2rD777DMtX75cDRs2zHe5vI5YJCQkKDk5udJ23q6I3G63kpLM0TvsNCxgVUZN7Me8CrVHSUkb1bNna3Xv7qzUYUoy9zmvo652U1rvlzlzch+Nq1fPPBo3cKDf//WWibLaZzv/DaPOJvbZPkr7/ZKWlqaYmBj7d942DEN33nmn5syZo6VLlxYYKiTJ5XLJ5XLlmh8UFGS7PzSgLnZETewjKEi64gopI2OfrriiXZWoS1CQ1KOHv1tRdFbfL0OGmMPo5rxSr0NOp23OQi51Zb3PdvwbVpXrvGRJlhYsWKc+fc7X5ZcHVol9rkh1Lq33S3G24dffxpgxYzRz5kx99tlnqlatmg4cOCBJioqKUmhoaCFrAwBgb05n5emgXVTsc9XgdErduhk6cWKfunVrZ8ujj6WtKta5uEpx8Mjie/XVV5Wamqru3bsrPj7eN3344Yf+bBYAAACAYvL7qVAAAAAAKj6/HrEAAAAAUDkQLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWObXYLF8+XL1799fderUkcPh0Ny5c/3ZHAAAAAAl5NdgceLECbVr104vv/yyP5sBAAAAwKJAfz55nz591KdPH382AQAAAEApoI8FAAAAAMv8esSiuDIyMpSRkeG7n5aWJklyu91yu93+ahbOkV0LamIf1MSeqIs9URf7oSb2RF3sqbTrUpztOAzDMErlWS1yOByaM2eOrr766nyXmThxoiZNmpRr/syZMxUWFlaGrQMAAACqnvT0dA0fPlypqamKjIwscNkKFSzyOmKRkJCg5ORkRUdHl0MrURRut1tJSUnq2bOngoKC/N0ciJrYFXWxJ+piP9TEnqiLPZV2XdLS0hQTE1OkYFGhToVyuVxyuVy55gcFBfGCtiHqYj/UxJ6oiz1RF/uhJvZEXeyptOpSnG34NVgcP35c27dv993fsWOH1q1bp5o1a6p+/fp+bBkAAACA4vBrsFizZo0uv/xy3/3x48dLkkaOHKkZM2b4qVUAAAAAisuvwaJ79+6ySRcPAAAAABZwHQsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYZotg8fLLLysxMVEhISG66KKLtGrVKn83CQAAAEAx+D1YfPjhhxo/frwmTJign3/+We3atVOvXr2UkpLi76YBAAAAKCK/B4upU6dq9OjRuvHGG9WyZUu99tprCgsL03/+8x9/Nw0AAABAEfk1WGRmZuqnn35Sjx49fPMCAgLUo0cPrVixwo8tAwAAAFAcgf588kOHDsnj8ah27do55teuXVu//fZbruUzMjKUkZHhu5+amipJ+vPPP8u2oSgWt9ut9PR0HT58WEFBQf5uDkRN7Iq62BN1sR9qYk/UxZ5Kuy7Hjh2TJBmGUeiyfg0WxTV58mRNmjQp1/xmzZr5oTUAAABA1XDs2DFFRUUVuIxfg0VMTIycTqcOHjyYY/7BgwcVFxeXa/mHH35Y48eP990/evSoGjRooN27dxe6oyg/aWlpSkhI0J49exQZGenv5kDUxK6oiz1RF/uhJvZEXeyptOtiGIaOHTumOnXqFLqsX4NFcHCw2rdvr8WLF+vqq6+WJHm9Xi1evFhjx47NtbzL5ZLL5co1Pyoqihe0DUVGRlIXm6Em9kRd7Im62A81sSfqYk+lWZeifoHv91Ohxo8fr5EjR6pDhw7q1KmTpk2bphMnTujGG2/0d9MAAAAAFJHfg8XQoUP1xx9/6PHHH9eBAwd0/vnn66uvvsrVoRsAAACAffk9WEjS2LFj8zz1qTAul0sTJkzI8/Qo+A91sR9qYk/UxZ6oi/1QE3uiLvbkz7o4jKKMHQUAAAAABfD7lbcBAAAAVHwECwAAAACWESwAAAAAWFahg8XLL7+sxMREhYSE6KKLLtKqVav83aQqY+LEiXI4HDmm5s2b+x4/deqUxowZo+joaEVERGjw4MG5LoQI65YvX67+/furTp06cjgcmjt3bo7HDcPQ448/rvj4eIWGhqpHjx7atm1bjmX+/PNPjRgxQpGRkapevbpuvvlmHT9+vBz3onIprCajRo3K9d7p3bt3jmWoSembPHmyOnbsqGrVqqlWrVq6+uqrtWXLlhzLFOXv1u7du9WvXz+FhYWpVq1auv/++5WVlVWeu1JpFKUm3bt3z/V+ue2223IsQ01K16uvvqq2bdv6roHQuXNnLViwwPc47xP/KKwudnmvVNhg8eGHH2r8+PGaMGGCfv75Z7Vr1069evVSSkqKv5tWZbRq1UrJycm+6bvvvvM9ds899+jzzz/Xxx9/rGXLlmn//v0aNGiQH1tbOZ04cULt2rXTyy+/nOfjzz77rP7973/rtdde048//qjw8HD16tVLp06d8i0zYsQIbdq0SUlJSfriiy+0fPly3XrrreW1C5VOYTWRpN69e+d478yaNSvH49Sk9C1btkxjxozRypUrlZSUJLfbrSuvvFInTpzwLVPY3y2Px6N+/fopMzNTP/zwg9555x3NmDFDjz/+uD92qcIrSk0kafTo0TneL88++6zvMWpS+urVq6dnnnlGP/30k9asWaO//OUvGjBggDZt2iSJ94m/FFYXySbvFaOC6tSpkzFmzBjffY/HY9SpU8eYPHmyH1tVdUyYMMFo165dno8dPXrUCAoKMj7++GPfvM2bNxuSjBUrVpRTC6seScacOXN8971erxEXF2c899xzvnlHjx41XC6XMWvWLMMwDOPXX381JBmrV6/2LbNgwQLD4XAY+/btK7e2V1bn1sQwDGPkyJHGgAED8l2HmpSPlJQUQ5KxbNkywzCK9ndr/vz5RkBAgHHgwAHfMq+++qoRGRlpZGRklO8OVELn1sQwDKNbt27G3Xffne861KR81KhRw3jrrbd4n9hMdl0Mwz7vlQp5xCIzM1M//fSTevTo4ZsXEBCgHj16aMWKFX5sWdWybds21alTR40aNdKIESO0e/duSdJPP/0kt9udoz7NmzdX/fr1qU852rFjhw4cOJCjDlFRUbrooot8dVixYoWqV6+uDh06+Jbp0aOHAgIC9OOPP5Z7m6uKpUuXqlatWjrvvPN0++236/Dhw77HqEn5SE1NlSTVrFlTUtH+bq1YsUJt2rTJcQHXXr16KS0tLce3hiiZc2uS7f3331dMTIxat26thx9+WOnp6b7HqEnZ8ng8+uCDD3TixAl17tyZ94lNnFuXbHZ4r9jiAnnFdejQIXk8nlxX565du7Z+++03P7Wqarnooos0Y8YMnXfeeUpOTtakSZPUtWtXbdy4UQcOHFBwcLCqV6+eY53atWvrwIED/mlwFZT9u87rfZL92IEDB1SrVq0cjwcGBqpmzZrUqoz07t1bgwYNUsOGDfX777/rkUceUZ8+fbRixQo5nU5qUg68Xq/GjRunLl26qHXr1pJUpL9bBw4cyPP9lP0YSi6vmkjS8OHD1aBBA9WpU0e//PKLHnzwQW3ZskWzZ8+WRE3KyoYNG9S5c2edOnVKERERmjNnjlq2bKl169bxPvGj/Ooi2ee9UiGDBfyvT58+vttt27bVRRddpAYNGuijjz5SaGioH1sG2Nt1113nu92mTRu1bdtWjRs31tKlS3XFFVf4sWVVx5gxY7Rx48Yc/cLgX/nV5Oy+RW3atFF8fLyuuOIK/f7772rcuHF5N7PKOO+887Ru3Tqlpqbqk08+0ciRI7Vs2TJ/N6vKy68uLVu2tM17pUKeChUTEyOn05lrFIKDBw8qLi7OT62q2qpXr65mzZpp+/btiouLU2Zmpo4ePZpjGepTvrJ/1wW9T+Li4nINeJCVlaU///yTWpWTRo0aKSYmRtu3b5dETcra2LFj9cUXX2jJkiWqV6+eb35R/m7FxcXl+X7Kfgwlk19N8nLRRRdJUo73CzUpfcHBwWrSpInat2+vyZMnq127dnrhhRd4n/hZfnXJi7/eKxUyWAQHB6t9+/ZavHixb57X69XixYtznGuG8nP8+HH9/vvvio+PV/v27RUUFJSjPlu2bNHu3bupTzlq2LCh4uLictQhLS1NP/74o68OnTt31tGjR/XTTz/5lvnmm2/k9Xp9f5RQtvbu3avDhw8rPj5eEjUpK4ZhaOzYsZozZ46++eYbNWzYMMfjRfm71blzZ23YsCFH8EtKSlJkZKTvdAQUXWE1ycu6deskKcf7hZqUPa/Xq4yMDN4nNpNdl7z47b1Sat3Ay9kHH3xguFwuY8aMGcavv/5q3HrrrUb16tVz9HZH2bn33nuNpUuXGjt27DC+//57o0ePHkZMTIyRkpJiGIZh3HbbbUb9+vWNb775xlizZo3RuXNno3Pnzn5udeVz7NgxY+3atcbatWsNScbUqVONtWvXGrt27TIMwzCeeeYZo3r16sZnn31m/PLLL8aAAQOMhg0bGidPnvRto3fv3sYFF1xg/Pjjj8Z3331nNG3a1Bg2bJi/dqnCK6gmx44dM+677z5jxYoVxo4dO4xFixYZF154odG0aVPj1KlTvm1Qk9J3++23G1FRUcbSpUuN5ORk35Senu5bprC/W1lZWUbr1q2NK6+80li3bp3x1VdfGbGxscbDDz/sj12q8Aqryfbt240nnnjCWLNmjbFjxw7js88+Mxo1amRcdtllvm1Qk9L30EMPGcuWLTN27Nhh/PLLL8ZDDz1kOBwO4+uvvzYMg/eJvxRUFzu9VypssDAMw3jxxReN+vXrG8HBwUanTp2MlStX+rtJVcbQoUON+Ph4Izg42Khbt64xdOhQY/v27b7HT548adxxxx1GjRo1jLCwMGPgwIFGcnKyH1tcOS1ZssSQlGsaOXKkYRjmkLOPPfaYUbt2bcPlchlXXHGFsWXLlhzbOHz4sDFs2DAjIiLCiIyMNG688Ubj2LFjftibyqGgmqSnpxtXXnmlERsbawQFBRkNGjQwRo8enesLEWpS+vKqiSRj+vTpvmWK8ndr586dRp8+fYzQ0FAjJibGuPfeew23213Oe1M5FFaT3bt3G5dddplRs2ZNw+VyGU2aNDHuv/9+IzU1Ncd2qEnpuummm4wGDRoYwcHBRmxsrHHFFVf4QoVh8D7xl4LqYqf3isMwDKP0jn8AAAAAqIoqZB8LAAAAAPZCsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAJTIH3/8odtvv13169eXy+VSXFycevXqpe+//16S5HA4NHfuXP82EgBQbgL93QAAQMU0ePBgZWZm6p133lGjRo108OBBLV68WIcPH/Z30wAAfuAwDMPwdyMAABXL0aNHVaNGDS1dulTdunXL9XhiYqJ27drlu9+gQQPt3LlTkvTZZ59p0qRJ+vXXX1WnTh2NHDlSjz76qAIDze+6HA6HXnnlFc2bN09Lly5VfHy8nn32WV1zzTXlsm8AgJLhVCgAQLFFREQoIiJCc+fOVUZGRq7HV69eLUmaPn26kpOTffe//fZb3XDDDbr77rv166+/6vXXX9eMGTP09NNP51j/scce0+DBg7V+/XqNGDFC1113nTZv3lz2OwYAKDGOWAAASuTTTz/V6NGjdfLkSV144YXq1q2brrvuOrVt21aSeeRhzpw5uvrqq33r9OjRQ1dccYUefvhh37z//ve/euCBB7R//37ferfddpteffVV3zIXX3yxLrzwQr3yyivls3MAgGLjiAUAoEQGDx6s/fv3a968eerdu7eWLl2qCy+8UDNmzMh3nfXr1+uJJ57wHfGIiIjQ6NGjlZycrPT0dN9ynTt3zrFe586dOWIBADZH520AQImFhISoZ8+e6tmzpx577DHdcsstmjBhgkaNGpXn8sePH9ekSZM0aNCgPLcFAKi4OGIBACg1LVu21IkTJyRJQUFB8ng8OR6/8MILtWXLFjVp0iTXFBBw5l/SypUrc6y3cuVKtWjRoux3AABQYhyxAAAU2+HDh3XttdfqpptuUtu2bVWtWjWtWbNGzz77rAYMGCDJHBlq8eLF6tKli1wul2rUqKHHH39cV111lerXr69rrrlGAQEBWr9+vTZu3KinnnrKt/2PP/5YHTp00KWXXqr3339fq1at0ttvv+2v3QUAFAGdtwEAxZaRkaGJEyfq66+/1u+//y63262EhARde+21euSRRxQaGqrPP/9c48eP186dO1W3bl3fcLMLFy7UE088obVr1yooKEjNmzfXLbfcotGjR0syO2+//PLLmjt3rpYvX674+HhNmTJFQ4YM8eMeAwAKQ7AAANhKXqNJAQDsjz4WAAAAACwjWAAAAACwjM7bAABb4QxdAKiYOGIBAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMCy/weUH/d5ev2DCQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "steps = [30, 60, 90, 120,150,180, 210, 240, 270, 300, 330]\n",
    "training_loss = [8.301000, 7.860100, 8.031400, 7.999300, 8.024700, 7.465600, 7.805800, 7.330000, 7.467800,7.31900,7.183200]\n",
    "validation_loss = [2.140013, 2.111856, 2.116166, 2.103918, 2.089628, 2.075586, 2.069516, 2.064974, 2.06254, 2.061258, 2.059105]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, training_loss, color='orange', label='Training Loss', marker='o')\n",
    "plt.plot(steps, validation_loss, color='blue', label='Validation Loss', marker='o')\n",
    "\n",
    "# Axis ranges\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 9)\n",
    "\n",
    "# Labels and grid\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer : Cell 9 should be executed before Cell 10 , i made a mistake of deledting the trainer aftersaving the model - the code is now correct you just need to run cell by cell  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9: Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "try:\n",
    "    perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "except:\n",
    "    print(\"Could not calculate perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 10: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = \"/kaggle/working/final_model\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "\n",
    "# Save training state\n",
    "trainer.save_state()\n",
    "print(\"Training state saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-10T23:23:17.846415Z",
     "iopub.status.idle": "2025-07-10T23:23:17.846634Z",
     "shell.execute_reply": "2025-07-10T23:23:17.846543Z",
     "shell.execute_reply.started": "2025-07-10T23:23:17.846534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "# del trainer\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment - You can run this after Cell 10 but you should have skipped cell 9 , and accidently deleted the trainer , then only , or just simple follow the cell by cell execution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T23:57:51.748186Z",
     "iopub.status.busy": "2025-07-10T23:57:51.747768Z",
     "iopub.status.idle": "2025-07-10T23:58:33.063895Z",
     "shell.execute_reply": "2025-07-10T23:58:33.063271Z",
     "shell.execute_reply.started": "2025-07-10T23:57:51.748162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 23:57:56.508116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752191876.530634     666 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752191876.537515     666 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.7.2: Fast Llama patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Load the base model\n",
    "model_name = \"unsloth/llama-3-8b-bnb-4bit\"  # Change as needed\n",
    "max_seq_length = 2048                 \n",
    "dtype = None  # Auto-detect  # dtype = torch.float16\n",
    "load_in_4bit = True  # load_in_4bit = False\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load your trained adapters from local path\n",
    "model = PeftModel.from_pretrained(model, \"/kaggle/working/final_model\")\n",
    "\n",
    "# Prepare model for inference and merge\n",
    "FastLanguageModel.for_inference(model)\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T00:04:00.955890Z",
     "iopub.status.busy": "2025-07-11T00:04:00.955588Z",
     "iopub.status.idle": "2025-07-11T00:05:37.720159Z",
     "shell.execute_reply": "2025-07-11T00:05:37.719400Z",
     "shell.execute_reply.started": "2025-07-11T00:04:00.955872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading your exact eval dataset...\n",
      "Dataset loaded: 1533 samples\n",
      "After filtering: 1533 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a91b8829ca4905a48cb1f2bca7c620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for split:   0%|          | 0/1533 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eval dataset recreated: 154 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b5214290f14485bd5b16efdc060e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1533 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 100 texts from your exact eval split\n",
      "\n",
      "ğŸ” Calculating perplexity...\n",
      "Processing text 1/100\n",
      "Processing text 11/100\n",
      "Processing text 21/100\n",
      "Processing text 31/100\n",
      "Processing text 41/100\n",
      "Processing text 51/100\n",
      "Processing text 61/100\n",
      "Processing text 71/100\n",
      "Processing text 81/100\n",
      "Processing text 91/100\n",
      "\n",
      "ğŸ“Š PERPLEXITY RESULTS:\n",
      "Perplexity: 9.4790\n",
      "Calculated on 51,100 tokens\n",
      "Average tokens per text: 511.0\n",
      "\n",
      "ğŸ“ˆ Interpretation:\n",
      "âœ… Excellent - Very low perplexity (< 10)\n",
      "ğŸ’¾ Results saved to /kaggle/working/perplexity_results.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================= PERPLEXITY CALCULATION =================\n",
    "def calculate_perplexity_unsloth(model, tokenizer, texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of texts using the merged model\n",
    "    \"\"\"\n",
    "    total_log_likelihood = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processing text {i+1}/{len(texts)}\")\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=max_length, \n",
    "                truncation=True, \n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs.get(\"attention_mask\", None)\n",
    "            \n",
    "            # Skip if too short\n",
    "            if input_ids.shape[1] < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Shift logits and labels for causal LM\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            \n",
    "            # Calculate loss only for non-padded tokens\n",
    "            if attention_mask is not None:\n",
    "                shift_attention_mask = attention_mask[..., 1:].contiguous()\n",
    "                active_tokens = shift_attention_mask.view(-1) == 1\n",
    "                active_logits = shift_logits.view(-1, shift_logits.size(-1))[active_tokens]\n",
    "                active_labels = shift_labels.view(-1)[active_tokens]\n",
    "            else:\n",
    "                active_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                active_labels = shift_labels.view(-1)\n",
    "            \n",
    "            # Calculate cross entropy loss\n",
    "            loss = F.cross_entropy(active_logits, active_labels, reduction='sum')\n",
    "            \n",
    "            total_log_likelihood += loss.item()\n",
    "            total_tokens += len(active_labels)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    if total_tokens == 0:\n",
    "        return float('inf'), 0\n",
    "    \n",
    "    avg_log_likelihood = total_log_likelihood / total_tokens\n",
    "    perplexity = np.exp(avg_log_likelihood)\n",
    "    \n",
    "    return perplexity, total_tokens\n",
    "\n",
    "# Load your exact eval dataset (same split as training)\n",
    "from datasets import load_dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Simple tokenization function for splitting dataset\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048,\n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "\n",
    "print(\"ğŸ”„ Loading your exact eval dataset...\")\n",
    "try:\n",
    "    # Load the same dataset\n",
    "    dataset_name = \"Harshu0117/AKS_IISC_1024_processed\"\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    # Apply the same filtering\n",
    "    dataset = dataset.filter(lambda x: x[\"text\"] and len(x[\"text\"].strip()) > 10)\n",
    "    print(f\"After filtering: {len(dataset)} samples\")\n",
    "    \n",
    "    # Tokenize dataset (needed for the split)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"Tokenizing for split\",\n",
    "    )\n",
    "    \n",
    "    # Split with the SAME seed=42 to get identical eval set\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "    \n",
    "    print(f\"âœ… Eval dataset recreated: {len(eval_dataset)} samples\")\n",
    "    \n",
    "    # Get original text from the eval split\n",
    "    # We need to get the text from the original dataset using the same indices\n",
    "    original_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    original_dataset = original_dataset.filter(lambda x: x[\"text\"] and len(x[\"text\"].strip()) > 10)\n",
    "    \n",
    "    # Get the same split indices\n",
    "    original_split = original_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    eval_texts_dataset = original_split[\"test\"]\n",
    "    \n",
    "    # Extract text for perplexity calculation\n",
    "    test_texts = eval_texts_dataset[\"text\"][:100]  # Use first 100 for faster calculation\n",
    "    print(f\"âœ… Using {len(test_texts)} texts from your exact eval split\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading eval dataset: {e}\")\n",
    "    print(\"âš ï¸ Using fallback example texts\")\n",
    "    test_texts = [\n",
    "        \"Materials science is the study of the properties and applications of materials.\",\n",
    "        \"The microstructure of materials determines their mechanical properties.\",\n",
    "        \"Crystal structures play a crucial role in determining material behavior.\",\n",
    "        \"Polymers are large molecules composed of repeated subunits.\",\n",
    "        \"Metals exhibit high electrical and thermal conductivity.\",\n",
    "    ]\n",
    "\n",
    "print(\"\\nğŸ” Calculating perplexity...\")\n",
    "try:\n",
    "    perplexity, num_tokens = calculate_perplexity_unsloth(merged_model, tokenizer, test_texts)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š PERPLEXITY RESULTS:\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "    print(f\"Calculated on {num_tokens:,} tokens\")\n",
    "    print(f\"Average tokens per text: {num_tokens/len(test_texts):.1f}\")\n",
    "    \n",
    "    # Interpretation guide\n",
    "    print(f\"\\nğŸ“ˆ Interpretation:\")\n",
    "    if perplexity < 10:\n",
    "        print(\"âœ… Excellent - Very low perplexity (< 10)\")\n",
    "    elif perplexity < 30:\n",
    "        print(\"âœ… Good - Low perplexity (10-30)\") \n",
    "    elif perplexity < 100:\n",
    "        print(\"âš ï¸ Moderate - Acceptable perplexity (30-100)\")\n",
    "    else:\n",
    "        print(\"âŒ High - Model may need more training (> 100)\")\n",
    "        \n",
    "    # Save results\n",
    "    with open(\"/kaggle/working/perplexity_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Perplexity: {perplexity:.4f}\\n\")\n",
    "        f.write(f\"Tokens: {num_tokens:,}\\n\")\n",
    "        f.write(f\"Texts evaluated: {len(test_texts)}\\n\")\n",
    "    \n",
    "    print(f\"ğŸ’¾ Results saved to /kaggle/working/perplexity_results.txt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error calculating perplexity: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# # Memory cleanup before pushing\n",
    "# print(\"\\nğŸ§¹ Cleaning up memory...\")\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T00:07:03.041247Z",
     "iopub.status.busy": "2025-07-11T00:07:03.040647Z",
     "iopub.status.idle": "2025-07-11T00:07:59.240596Z",
     "shell.execute_reply": "2025-07-11T00:07:59.239892Z",
     "shell.execute_reply.started": "2025-07-11T00:07:03.041220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Now proceeding with model upload...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370d15d3a864c64afea0536786244f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a09ed8afe444f495faea716991a0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c402b9238734946bcdb9aea94ae8cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd116b415cd4e4f8b6cd5cf9019cb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Harshu0117/Materials_IISC_MRC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ef958ac9843139647b63be266934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a5432f66754cf190dcabd99b8198a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete merged model pushed successfully!\n",
      "ğŸ”— Repository: https://huggingface.co/Harshu0117/Materials_IISC_MRC\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸš€ Now proceeding with model upload...\")\n",
    "\n",
    "# Push complete merged model to Hugging Face Hub\n",
    "merged_model.push_to_hub(\n",
    "    \"Harshu0117/Materials_IISC_MRC\",\n",
    "    token=\"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\",  # Replace with your actual token\n",
    "    private=False,  # Set to True if you want private repo\n",
    "    safe_serialization=True\n",
    ")\n",
    "\n",
    "# Push tokenizer\n",
    "tokenizer.push_to_hub(\n",
    "    \"Harshu0117/Materials_IISC_MRC\",\n",
    "    token=\"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\",  # Replace with your actual token\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Complete merged model pushed successfully!\")\n",
    "print(\"ğŸ”— Repository: https://huggingface.co/Harshu0117/Materials_IISC_MRC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrenceing the model - importing from the HF space,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T00:16:27.515522Z",
     "iopub.status.busy": "2025-07-11T00:16:27.514932Z",
     "iopub.status.idle": "2025-07-11T00:17:43.587714Z",
     "shell.execute_reply": "2025-07-11T00:17:43.587032Z",
     "shell.execute_reply.started": "2025-07-11T00:16:27.515495Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0430f6512194403db8c7a1e77107c07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399d87fe0834547a4d891ebb882d344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e03a10cbaa4c14808bc3d6d2527817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d4e3456fbd49849984d8a99c0cdb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 00:16:37.707146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752192997.726659    1153 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752192997.731821    1153 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b59a4113b6495bad6dd02602a27e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e893108d1354ac29a0ec3c5ce6f185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f091c294f8264d9fa14501e2bf96bc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c4614d292a455baeb6d80d36ddc810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825c66a5fd947d6bdf32fe34cb502a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d3a91811604a359623f93dd3539371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load your model from Hugging Face\n",
    "model_name = \"Harshu0117/Materials_IISC_MRC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T00:17:43.590133Z",
     "iopub.status.busy": "2025-07-11T00:17:43.589057Z",
     "iopub.status.idle": "2025-07-11T00:18:02.442362Z",
     "shell.execute_reply": "2025-07-11T00:18:02.439606Z",
     "shell.execute_reply.started": "2025-07-11T00:17:43.590108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crystalline MAX Phases and their 2D derivative MXenes are two-dimensional transition metal carbides with excellent physical properties. They can be used as a catalyst, electrode material for supercapacitors or fuel cells etc.\n",
      "The production of crystalline MAX phases is usually done by sintering the raw materials (MgB _{4}, ZrSiO _{3} ) at high temperatures up to ~2000Â°C in vacuum furnaces without any reaction control system. During this process, several gases such as SiO(g) and BCl(g) are formed which have very different thermal conductivity than air leading to large temperature gradients within the furnace chamber. It was shown that these temperature differences could lead to an explosion like failure if proper precautions were not taken [1].\n",
      "In collaboration with Fraunhofer Institute IWM in Freiburg i.Br., we investigate the formation mechanism during reactive sputtering experiments [2]. The results show that under certain conditions it is possible to achieve stable growth rates even on substrates exposed to water vapor but only\n"
     ]
    }
   ],
   "source": [
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "   tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Inference\n",
    "inputs = tokenizer(\"Crystalline MAX Phases and their 2D derivative MXenes\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "   **inputs,\n",
    "   max_new_tokens=200,\n",
    "   repetition_penalty=1.2,\n",
    "   temperature=0.8,\n",
    "   top_k=50,\n",
    "   top_p=0.95,\n",
    "   do_sample=True,\n",
    "   pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
